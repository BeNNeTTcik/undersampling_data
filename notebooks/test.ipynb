{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e131a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import gower\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "#from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    151\n",
      "1     46\n",
      "Name: count, dtype: int64\n",
      "197\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\original_data.csv\")    \n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote_data.csv\")\n",
    "GAN_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN_data.csv\")\n",
    "borderline_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline_data.csv\")\n",
    "smote2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote3_data.csv\")\n",
    "GAN2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN3_data.csv\")    \n",
    "borderline2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline3_data.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\y_test.csv\")\n",
    "\n",
    "# load scalar and encoder map \n",
    "with open(\"D:\\\\ml\\\\undersampling_data\\\\models\\\\scaler_ip_no_td.pkl\", \"rb\") as f:\n",
    "    scalar_ip_no_td = pickle.load(f)\n",
    "with open(\"D:\\\\ml\\\\undersampling_data\\\\models\\\\map_user.pkl\", \"rb\") as f:\n",
    "    map_user = pickle.load(f)\n",
    "\n",
    "#Before undersampling\n",
    "print(original_data[\"target\"].value_counts())\n",
    "count1=original_data[\"target\"].value_counts().sum()\n",
    "print(count1)\n",
    "count2=abs((original_data['target']==0).sum() - (original_data['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b35c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed data\n",
    "mix_data = pd.concat([GAN_data, smote_data, borderline_data], axis=0, ignore_index=True)         \n",
    "mix_data = mix_data.reset_index(drop=True)\n",
    "\n",
    "#data with one oversampling method and original data e.g.(smote+original)\n",
    "smote_data = pd.concat([smote_data, smote2_data], axis=0, ignore_index=True)\n",
    "smote_data = smote_data.reset_index(drop=True)\n",
    "borderline_data = pd.concat([borderline_data, borderline2_data], axis=0, ignore_index=True)\n",
    "borderline_data = borderline_data.reset_index(drop=True)\n",
    "GAN_data = pd.concat([GAN_data, GAN2_data], axis=0, ignore_index=True)\n",
    "GAN_data = GAN_data.reset_index(drop=True)\n",
    "\n",
    "#convert data types to float64\n",
    "int_cols = mix_data.select_dtypes(include=[\"int\"]).columns\n",
    "mix_data[int_cols] = mix_data[int_cols].astype(\"float64\")\n",
    "int_cols = smote_data.select_dtypes(include=[\"int\"]).columns\n",
    "smote_data[int_cols] = smote_data[int_cols].astype(\"float64\")\n",
    "int_cols = borderline_data.select_dtypes(include=[\"int\"]).columns\n",
    "borderline_data[int_cols] = borderline_data[int_cols].astype(\"float64\")\n",
    "int_cols = GAN_data.select_dtypes(include=[\"int\"]).columns\n",
    "GAN_data[int_cols] = GAN_data[int_cols].astype(\"float64\")\n",
    "\n",
    "sum_all_data = pd.concat([smote_data, GAN_data, borderline_data, original_data], axis=0, ignore_index=True)\n",
    "sum_all_data = sum_all_data.drop_duplicates()\n",
    "\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_data.drop(columns=[\"target\", \"source\"]), mix_data[\"target\"]\n",
    "X_smote, y_smote = smote_data.drop(columns=[\"target\", \"source\"]), smote_data[\"target\"]\n",
    "X_GAN, y_GAN = GAN_data.drop(columns=[\"target\", \"source\"]), GAN_data[\"target\"]\n",
    "X_borderline, y_borderline = borderline_data.drop(columns=[\"target\", \"source\"]), borderline_data[\"target\"]\n",
    "\n",
    "#dodac standrazycje \n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_data\n",
    "compare[\"smote\"] = smote_data\n",
    "compare[\"GAN\"] = GAN_data\n",
    "compare[\"borderline\"] = borderline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300eef4",
   "metadata": {},
   "source": [
    "Dodatkowo mozna sprawdzic dzialanie \"init=\"k-means++\"\" na wyniki\n",
    "\n",
    "Stworzenie mixa mteryk i potem sprowadzenie do wartosci liniowej.\n",
    "\n",
    "- wyprobowanie gower distance na moich danych\n",
    "\n",
    "- zmiana danych (onehot endocer dla user oraz td zamiana na log1p(td))tak aby dalo sie wykorzystac metryki euclidean, cosinus similarity itd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab11a5",
   "metadata": {},
   "source": [
    "### Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c26ba33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kamran' 'student' 'root' 'admins' 'phoenix' 'piglet' 'rainbow' 'runner'\n",
      " 'sam' 'abc123' 'passwd' 'newpass' 'notused' 'Hockey' 'internet' 'asshole'\n",
      " 'Maddock' 'computer' 'Mickey' 'qwerty' 'fiction' 'orange' 'tigger'\n",
      " 'wheeling' 'mustang' 'admin' 'jennifer' 'money' 'Justin' 'chris' 'david'\n",
      " 'foobar' 'buster' 'harley' 'jordan' 'stupid' 'apple' 'fred' 'summer'\n",
      " 'sunshine' 'andrew' 'osamac' 'gta' 'adminx' 'gtta' 'osamax']\n",
      "{0: 'kamran', 1: 'student', 2: 'root', 3: 'admins', 4: 'phoenix', 5: 'piglet', 6: 'rainbow', 7: 'runner', 8: 'sam', 9: 'abc123', 10: 'passwd', 11: 'newpass', 12: 'notused', 13: 'Hockey', 14: 'internet', 15: 'asshole', 16: 'Maddock', 17: 'computer', 18: 'Mickey', 19: 'qwerty', 20: 'fiction', 21: 'orange', 22: 'tigger', 23: 'wheeling', 24: 'mustang', 25: 'admin', 26: 'jennifer', 27: 'money', 28: 'Justin', 29: 'chris', 30: 'david', 31: 'foobar', 32: 'buster', 33: 'harley', 34: 'jordan', 35: 'stupid', 36: 'apple', 37: 'fred', 38: 'summer', 39: 'sunshine', 40: 'andrew', 41: 'osamac', 42: 'gta', 43: 'adminx', 44: 'gtta', 45: 'osamax'}\n",
      "         0      2      3      4      5      7      8     11     12     14  \\\n",
      "0    False  False  False  False  False  False  False  False  False  False   \n",
      "1    False  False  False  False  False  False  False  False  False  False   \n",
      "2    False  False  False  False  False  False  False  False  False  False   \n",
      "3    False  False  False  False  False  False  False  False  False  False   \n",
      "4    False  False  False  False  False  False  False  False   True  False   \n",
      "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "192  False  False  False  False  False  False  False  False  False  False   \n",
      "193  False  False  False  False  False  False  False  False  False  False   \n",
      "194  False  False  False  False  False  False  False  False  False  False   \n",
      "195  False  False  False  False  False  False  False  False  False  False   \n",
      "196  False  False  False  False  False  False  False  False  False  False   \n",
      "\n",
      "     ...  is_valid  not_valid_count  ip_failure  ip_success  no_failure  \\\n",
      "0    ...         0                7           7          14          15   \n",
      "1    ...         1                0           2           0           3   \n",
      "2    ...         1                0           0          26           0   \n",
      "3    ...         1                0           2           3           3   \n",
      "4    ...         0                1          32           0          32   \n",
      "..   ...       ...              ...         ...         ...         ...   \n",
      "192  ...         1                0           3           2           4   \n",
      "193  ...         1                0           0           4           0   \n",
      "194  ...         1                0           0           9           0   \n",
      "195  ...         1                0           0          33           0   \n",
      "196  ...         1                0           0          12           0   \n",
      "\n",
      "     first   td        ts  target    source  \n",
      "0        0   12  0.539531       0  original  \n",
      "1        0   11  0.530035       0  original  \n",
      "2        0  900  0.540170       0  original  \n",
      "3        0    3  0.522882       0  original  \n",
      "4        0    3 -1.885636       1  original  \n",
      "..     ...  ...       ...     ...       ...  \n",
      "192      0    1  0.511124       0  original  \n",
      "193      0   26  0.512363       0  original  \n",
      "194      0    9  0.538242       0  original  \n",
      "195      0    8  0.540250       0  original  \n",
      "196      0   33  0.511790       0  original  \n",
      "\n",
      "[197 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "ssh_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\SSH.csv\")\n",
    "unique_users = ssh_df[\"user\"].unique()\n",
    "print(unique_users)\n",
    "user_map = {user: idx for idx, user in enumerate(unique_users)}\n",
    "list(user_map.items())[:15]\n",
    "\n",
    "user_dummies = pd.get_dummies(original_data[\"user\"], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "ssh_encoded = pd.concat([ user_dummies,original_data], axis=1)\n",
    "\n",
    "reverse_user_map = {v: k for k, v in user_map.items()}\n",
    "print(reverse_user_map)\n",
    "ssh_encoded = ssh_encoded.rename(columns=reverse_user_map)\n",
    "print(ssh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30dc7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Maddock  Mickey  admins  adminx  andrew  apple  asshole  buster  chris  \\\n",
      "0          0       0       0       0       0      0        0       0      0   \n",
      "1          0       0       0       0       1      0        0       0      0   \n",
      "2          0       0       0       0       0      0        0       0      0   \n",
      "3          0       0       0       0       0      0        0       0      0   \n",
      "4          0       0       0       0       0      0        0       0      0   \n",
      "..       ...     ...     ...     ...     ...    ...      ...     ...    ...   \n",
      "192        0       0       0       0       0      0        0       0      0   \n",
      "193        0       0       0       0       0      0        0       0      0   \n",
      "194        0       0       0       0       0      0        0       0      0   \n",
      "195        0       1       0       0       0      0        0       0      0   \n",
      "196        0       0       0       0       0      0        0       0      0   \n",
      "\n",
      "     computer  ...  is_valid  not_valid_count  ip_failure  ip_success  \\\n",
      "0           0  ...         0         0.900368    0.066760    0.671734   \n",
      "1           0  ...         1        -0.474612   -0.423975   -0.782460   \n",
      "2           0  ...         1        -0.474612   -0.620270    1.918186   \n",
      "3           0  ...         1        -0.474612   -0.423975   -0.470847   \n",
      "4           0  ...         0        -0.278187    2.520437   -0.782460   \n",
      "..        ...  ...       ...              ...         ...         ...   \n",
      "192         0  ...         1        -0.474612   -0.325828   -0.574718   \n",
      "193         0  ...         1        -0.474612   -0.620270   -0.366976   \n",
      "194         0  ...         1        -0.474612   -0.620270    0.152379   \n",
      "195         0  ...         1        -0.474612   -0.620270    2.645283   \n",
      "196         0  ...         1        -0.474612   -0.620270    0.463992   \n",
      "\n",
      "     no_failure  first        td        ts  target    source  \n",
      "0      0.699728      0 -0.148677  0.539531       0  original  \n",
      "1     -0.431706      0 -0.148712  0.530035       0  original  \n",
      "2     -0.714565      0 -0.118019  0.540170       0  original  \n",
      "3     -0.431706      0 -0.148988  0.522882       0  original  \n",
      "4      2.302592      0 -0.148988 -1.885636       1  original  \n",
      "..          ...    ...       ...       ...     ...       ...  \n",
      "192   -0.337420      0 -0.149057  0.511124       0  original  \n",
      "193   -0.714565      0 -0.148194  0.512363       0  original  \n",
      "194   -0.714565      0 -0.148781  0.538242       0  original  \n",
      "195   -0.714565      0 -0.148815  0.540250       0  original  \n",
      "196   -0.714565      0 -0.147952  0.511790       0  original  \n",
      "\n",
      "[197 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "def map_user_column(series, mapping):\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return series.map(mapping).astype(str)\n",
    "    else:\n",
    "        # spróbujmy rzutować na int\n",
    "        try:\n",
    "            as_int = series.astype(float).astype(int)\n",
    "            return as_int.map(mapping).astype(str)\n",
    "        except Exception:\n",
    "            # wygląda na to, że już są nazwy\n",
    "            return series.astype(str)\n",
    "        \n",
    "original_data[\"user\"] = map_user_column(original_data[\"user\"], reverse_user_map)\n",
    "user_dummies = pd.get_dummies(original_data[\"user\"], prefix=\"\", prefix_sep=\"\")\n",
    "ssh_encoded = pd.concat([ user_dummies,original_data], axis=1)\n",
    "ssh_encoded = ssh_encoded.replace({True: 1, False: 0})\n",
    "ssh_encoded = ssh_encoded.drop(columns=[\"user\"])\n",
    "\n",
    "scalar_ip_no_td = StandardScaler()\n",
    "cols_to_scale = [\"td\", \"ip_failure\", \"ip_success\", \"no_failure\", \"not_valid_count\"]\n",
    "ssh_encoded[cols_to_scale]= scalar_ip_no_td.fit_transform(ssh_encoded[cols_to_scale])\n",
    "ssh_encoded.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_normalized.csv\")\n",
    "with open(\"scaler_ip_no_td.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scalar_ip_no_td, f)\n",
    "with open(\"map_user.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reverse_user_map, f)\n",
    "\n",
    "print(ssh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafe74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maddock', 'Mickey', 'admins', 'adminx', 'andrew', 'apple', 'asshole', 'buster', 'chris', 'computer', 'david', 'fiction', 'foobar', 'fred', 'gta', 'gtta', 'harley', 'internet', 'jennifer', 'jordan', 'kamran', 'money', 'mustang', 'newpass', 'notused', 'orange', 'osamac', 'osamax', 'phoenix', 'piglet', 'qwerty', 'root', 'runner', 'sam', 'stupid', 'summer', 'tigger', 'wheeling', 'is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'ts', 'target', 'source']\n"
     ]
    }
   ],
   "source": [
    "ssh_encoded = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\ssh_encoded.csv\")\n",
    "ssh_encoded = ssh_encoded.drop(columns=[\"Unnamed: 0\"])\n",
    "cols = ssh_encoded.columns.tolist()\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcfeb388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     kamran  root  admins  phoenix  piglet  runner  sam  newpass  notused  \\\n",
      "0       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "1       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "2       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "3       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "4       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      1.0   \n",
      "..      ...   ...     ...      ...     ...     ...  ...      ...      ...   \n",
      "192     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "193     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "194     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "195     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "196     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "\n",
      "     internet  ...  stupid  apple  fred  summer  andrew  osamac  gta  adminx  \\\n",
      "0         0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "1         0.0  ...     0.0    0.0   0.0     0.0     1.0     0.0  0.0     0.0   \n",
      "2         0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "3         0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "4         0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "..        ...  ...     ...    ...   ...     ...     ...     ...  ...     ...   \n",
      "192       0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "193       0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "194       0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "195       0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "196       0.0  ...     0.0    0.0   0.0     0.0     0.0     0.0  0.0     0.0   \n",
      "\n",
      "     gtta  osamax  \n",
      "0     0.0     0.0  \n",
      "1     0.0     0.0  \n",
      "2     0.0     0.0  \n",
      "3     0.0     0.0  \n",
      "4     0.0     0.0  \n",
      "..    ...     ...  \n",
      "192   0.0     0.0  \n",
      "193   0.0     0.0  \n",
      "194   0.0     0.0  \n",
      "195   0.0     0.0  \n",
      "196   0.0     0.0  \n",
      "\n",
      "[197 rows x 38 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 197 entries, 0 to 196\n",
      "Data columns (total 51 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   kamran           197 non-null    float64\n",
      " 1   root             197 non-null    float64\n",
      " 2   admins           197 non-null    float64\n",
      " 3   phoenix          197 non-null    float64\n",
      " 4   piglet           197 non-null    float64\n",
      " 5   runner           197 non-null    float64\n",
      " 6   sam              197 non-null    float64\n",
      " 7   newpass          197 non-null    float64\n",
      " 8   notused          197 non-null    float64\n",
      " 9   internet         197 non-null    float64\n",
      " 10  asshole          197 non-null    float64\n",
      " 11  Maddock          197 non-null    float64\n",
      " 12  computer         197 non-null    float64\n",
      " 13  Mickey           197 non-null    float64\n",
      " 14  qwerty           197 non-null    float64\n",
      " 15  fiction          197 non-null    float64\n",
      " 16  orange           197 non-null    float64\n",
      " 17  tigger           197 non-null    float64\n",
      " 18  wheeling         197 non-null    float64\n",
      " 19  mustang          197 non-null    float64\n",
      " 20  jennifer         197 non-null    float64\n",
      " 21  money            197 non-null    float64\n",
      " 22  chris            197 non-null    float64\n",
      " 23  david            197 non-null    float64\n",
      " 24  foobar           197 non-null    float64\n",
      " 25  buster           197 non-null    float64\n",
      " 26  harley           197 non-null    float64\n",
      " 27  jordan           197 non-null    float64\n",
      " 28  stupid           197 non-null    float64\n",
      " 29  apple            197 non-null    float64\n",
      " 30  fred             197 non-null    float64\n",
      " 31  summer           197 non-null    float64\n",
      " 32  andrew           197 non-null    float64\n",
      " 33  osamac           197 non-null    float64\n",
      " 34  gta              197 non-null    float64\n",
      " 35  adminx           197 non-null    float64\n",
      " 36  gtta             197 non-null    float64\n",
      " 37  osamax           197 non-null    float64\n",
      " 38  is_private       197 non-null    int64  \n",
      " 39  is_failure       197 non-null    int64  \n",
      " 40  is_root          197 non-null    int64  \n",
      " 41  is_valid         197 non-null    int64  \n",
      " 42  not_valid_count  197 non-null    float64\n",
      " 43  ip_failure       197 non-null    float64\n",
      " 44  ip_success       197 non-null    float64\n",
      " 45  no_failure       197 non-null    float64\n",
      " 46  first            197 non-null    int64  \n",
      " 47  td               197 non-null    float64\n",
      " 48  ts               197 non-null    float64\n",
      " 49  target           197 non-null    int64  \n",
      " 50  source           197 non-null    object \n",
      "dtypes: float64(44), int64(6), object(1)\n",
      "memory usage: 78.6+ KB\n",
      "None\n",
      "     kamran  root  admins  phoenix  piglet  runner  sam  newpass  notused  \\\n",
      "0       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "1       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "2       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "3       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "4       0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      1.0   \n",
      "..      ...   ...     ...      ...     ...     ...  ...      ...      ...   \n",
      "192     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "193     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "194     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "195     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "196     0.0   0.0     0.0      0.0     0.0     0.0  0.0      0.0      0.0   \n",
      "\n",
      "     internet  ...  is_valid  not_valid_count  ip_failure  ip_success  \\\n",
      "0         0.0  ...         0         0.900368    0.066760    0.671734   \n",
      "1         0.0  ...         1        -0.474612   -0.423975   -0.782460   \n",
      "2         0.0  ...         1        -0.474612   -0.620270    1.918186   \n",
      "3         0.0  ...         1        -0.474612   -0.423975   -0.470847   \n",
      "4         0.0  ...         0        -0.278187    2.520437   -0.782460   \n",
      "..        ...  ...       ...              ...         ...         ...   \n",
      "192       0.0  ...         1        -0.474612   -0.325828   -0.574718   \n",
      "193       0.0  ...         1        -0.474612   -0.620270   -0.366976   \n",
      "194       0.0  ...         1        -0.474612   -0.620270    0.152379   \n",
      "195       0.0  ...         1        -0.474612   -0.620270    2.645283   \n",
      "196       0.0  ...         1        -0.474612   -0.620270    0.463992   \n",
      "\n",
      "     no_failure  first        td        ts  target    source  \n",
      "0      0.699728      0 -0.148677  0.539531       0  original  \n",
      "1     -0.431706      0 -0.148712  0.530035       0  original  \n",
      "2     -0.714565      0 -0.118019  0.540170       0  original  \n",
      "3     -0.431706      0 -0.148988  0.522882       0  original  \n",
      "4      2.302592      0 -0.148988 -1.885636       1  original  \n",
      "..          ...    ...       ...       ...     ...       ...  \n",
      "192   -0.337420      0 -0.149057  0.511124       0  original  \n",
      "193   -0.714565      0 -0.148194  0.512363       0  original  \n",
      "194   -0.714565      0 -0.148781  0.538242       0  original  \n",
      "195   -0.714565      0 -0.148815  0.540250       0  original  \n",
      "196   -0.714565      0 -0.147952  0.511790       0  original  \n",
      "\n",
      "[197 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "user_dummies = pd.get_dummies(original_data[\"user\"].astype(int), prefix=\"\", prefix_sep=\"\")\n",
    "def normalize_col(c):\n",
    "    s = str(c).strip()\n",
    "    return int(s) if s.isdigit() else s\n",
    "user_dummies = user_dummies.rename(columns=normalize_col)\n",
    "user_dummies = user_dummies.rename(columns=map_user)\n",
    "user_dummies = user_dummies.replace({True: 1.0, False: 0.0})\n",
    "print(user_dummies)\n",
    "data = pd.concat([ user_dummies,original_data], axis=1)\n",
    "data= data.drop(columns=[\"user\"])\n",
    "cols_to_scale = [\"td\", \"ip_failure\", \"ip_success\", \"no_failure\", \"not_valid_count\"]\n",
    "data[cols_to_scale]= scalar_ip_no_td.transform(data[cols_to_scale])\n",
    "print(data.info())\n",
    "print(data)\n",
    "data.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_data_normalized.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c85d0",
   "metadata": {},
   "source": [
    "### KMeans + centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    \n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_centroids_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a8f0b",
   "metadata": {},
   "source": [
    "### KMeans + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd840478",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_NN_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c21f6c",
   "metadata": {},
   "source": [
    "### KMeans + cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e2a8b",
   "metadata": {},
   "source": [
    "### K-means + cosinus + Mahalanobis distance (reduced to linear form, as it allows a simpler comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_MAN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5  #wazenie\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)   \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a47f7c",
   "metadata": {},
   "source": [
    "### HDBSCAN + Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e495470",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "labels_={}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels_[name] = len(unique_lables[unique_lables >=0])\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    \n",
    "    print(f\"{name}: {labels_[name]}\")\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "\n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_HDBSCAN_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_[\"smote\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42416291",
   "metadata": {},
   "source": [
    "### HDBSCAN + cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    print(labels)\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # policz dystanse do centroidu\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "       \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b791",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fcf38",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # policz dystanse do centroidu\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdc5d0",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_COS_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KMEANS_HDBSCAN_COS_NN_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1124158",
   "metadata": {},
   "source": [
    "#### -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2), init=\"k-means++\")\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    print(X_minority.shape)\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_COS_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(centroids_rows_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    print(df_majority.shape)  \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "    print(df_miniority.shape)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    print(df_[name].dtypes)\n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Num duplicates: {df_[name].duplicated().sum()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_SWAP_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(centroids_rows_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_SWAP_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "        \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(len(centroids_)):\n",
    "        \n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_HDBSCAN_DIST_[name].values(), ignore_index=True)\n",
    "        print(results_[name])\n",
    "        print(\"ilosc duplikaotow\",results_[name].duplicated().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
