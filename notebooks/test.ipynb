{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e131a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import gower\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "#from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f499689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    151\n",
      "1     46\n",
      "Name: count, dtype: int64\n",
      "197\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\original_data.csv\")    \n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote_data.csv\")\n",
    "GAN_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN_data.csv\")\n",
    "borderline_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline_data.csv\")\n",
    "smote2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote3_data.csv\")\n",
    "GAN2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN3_data.csv\")    \n",
    "borderline2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline3_data.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\y_test.csv\")\n",
    "\n",
    "# load scalar and encoder map \n",
    "with open(\"D:\\\\ml\\\\undersampling_data\\\\models\\\\scaler_ip_no_td.pkl\", \"rb\") as f:\n",
    "    scalar_ip_no_td = pickle.load(f)\n",
    "with open(\"D:\\\\ml\\\\undersampling_data\\\\models\\\\map_user.pkl\", \"rb\") as f:\n",
    "    map_user = pickle.load(f)\n",
    "\n",
    "#Before undersampling\n",
    "print(original_data[\"target\"].value_counts())\n",
    "count1=original_data[\"target\"].value_counts().sum()\n",
    "print(count1)\n",
    "count2=abs((original_data['target']==0).sum() - (original_data['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed data\n",
    "mix_data = pd.concat([GAN_data, smote_data, borderline_data], axis=0, ignore_index=True)         \n",
    "mix_data = mix_data.reset_index(drop=True)\n",
    "\n",
    "#data with one oversampling method and original data e.g.(smote+original)\n",
    "smote_data = pd.concat([smote_data, smote2_data], axis=0, ignore_index=True)\n",
    "smote_data = smote_data.reset_index(drop=True)\n",
    "borderline_data = pd.concat([borderline_data, borderline2_data], axis=0, ignore_index=True)\n",
    "borderline_data = borderline_data.reset_index(drop=True)\n",
    "GAN_data = pd.concat([GAN_data, GAN2_data], axis=0, ignore_index=True)\n",
    "GAN_data = GAN_data.reset_index(drop=True)\n",
    "\n",
    "#convert data types to float64\n",
    "int_cols = mix_data.select_dtypes(include=[\"int\"]).columns\n",
    "mix_data[int_cols] = mix_data[int_cols].astype(\"float64\")\n",
    "int_cols = smote_data.select_dtypes(include=[\"int\"]).columns\n",
    "smote_data[int_cols] = smote_data[int_cols].astype(\"float64\")\n",
    "int_cols = borderline_data.select_dtypes(include=[\"int\"]).columns\n",
    "borderline_data[int_cols] = borderline_data[int_cols].astype(\"float64\")\n",
    "int_cols = GAN_data.select_dtypes(include=[\"int\"]).columns\n",
    "GAN_data[int_cols] = GAN_data[int_cols].astype(\"float64\")\n",
    "\n",
    "sum_all_data = pd.concat([smote_data, GAN_data, borderline_data, original_data], axis=0, ignore_index=True)\n",
    "sum_all_data = sum_all_data.drop_duplicates()\n",
    "\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_data.drop(columns=[\"target\", \"source\"]), mix_data[\"target\"]\n",
    "X_smote, y_smote = smote_data.drop(columns=[\"target\", \"source\"]), smote_data[\"target\"]\n",
    "X_GAN, y_GAN = GAN_data.drop(columns=[\"target\", \"source\"]), GAN_data[\"target\"]\n",
    "X_borderline, y_borderline = borderline_data.drop(columns=[\"target\", \"source\"]), borderline_data[\"target\"]\n",
    "\n",
    "#dodac standrazycje \n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_data\n",
    "compare[\"smote\"] = smote_data\n",
    "compare[\"GAN\"] = GAN_data\n",
    "compare[\"borderline\"] = borderline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300eef4",
   "metadata": {},
   "source": [
    "Dodatkowo mozna sprawdzic dzialanie \"init=\"k-means++\"\" na wyniki\n",
    "\n",
    "Stworzenie mixa mteryk i potem sprowadzenie do wartosci liniowej.\n",
    "\n",
    "- wyprobowanie gower distance na moich danych\n",
    "\n",
    "- zmiana danych (onehot endocer dla user oraz td zamiana na log1p(td))tak aby dalo sie wykorzystac metryki euclidean, cosinus similarity itd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab11a5",
   "metadata": {},
   "source": [
    "### Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "unique_users = ssh_df[\"user\"].unique()\n",
    "print(unique_users)\n",
    "user_map = {user: idx for idx, user in enumerate(unique_users)}\n",
    "list(user_map.items())[:15]\n",
    "\n",
    "user_dummies = pd.get_dummies(original_data[\"user\"], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "ssh_encoded = pd.concat([ user_dummies,original_data], axis=1)\n",
    "\n",
    "reverse_user_map = {v: k for k, v in user_map.items()}\n",
    "print(reverse_user_map)\n",
    "ssh_encoded = ssh_encoded.rename(columns=reverse_user_map)\n",
    "print(ssh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_user_column(series, mapping):\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return series.map(mapping).astype(str)\n",
    "    else:\n",
    "        # spróbujmy rzutować na int\n",
    "        try:\n",
    "            as_int = series.astype(float).astype(int)\n",
    "            return as_int.map(mapping).astype(str)\n",
    "        except Exception:\n",
    "            # wygląda na to, że już są nazwy\n",
    "            return series.astype(str)\n",
    "        \n",
    "original_data[\"user\"] = map_user_column(original_data[\"user\"], reverse_user_map)\n",
    "user_dummies = pd.get_dummies(original_data[\"user\"], prefix=\"\", prefix_sep=\"\")\n",
    "ssh_encoded = pd.concat([ user_dummies,original_data], axis=1)\n",
    "ssh_encoded = ssh_encoded.replace({True: 1, False: 0})\n",
    "ssh_encoded = ssh_encoded.drop(columns=[\"user\"])\n",
    "\n",
    "scalar_ip_no_td = StandardScaler()\n",
    "cols_to_scale = [\"td\", \"ip_failure\", \"ip_success\", \"no_failure\", \"not_valid_count\"]\n",
    "ssh_encoded[cols_to_scale]= scalar_ip_no_td.fit_transform(ssh_encoded[cols_to_scale])\n",
    "ssh_encoded.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_normalized.csv\")\n",
    "with open(\"scaler_ip_no_td.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scalar_ip_no_td, f)\n",
    "with open(\"map_user.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reverse_user_map, f)\n",
    "\n",
    "print(ssh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bafe74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'ts']\n"
     ]
    }
   ],
   "source": [
    "ssh_encoded = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "ssh_encoded = ssh_encoded.drop(columns=[\"Unnamed: 0\"])\n",
    "cols = ssh_encoded.columns.tolist()\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfeb388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    student  root  piglet  rainbow  abc123  passwd  Hockey  Mickey  qwerty  \\\n",
      "0       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     1.0   \n",
      "1       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "..      ...   ...     ...      ...     ...     ...     ...     ...     ...   \n",
      "80      0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "81      0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "82      0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "83      0.0   0.0     1.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "84      0.0   0.0     0.0      0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "\n",
      "    mustang  admin  money  Justin  david  fred  sunshine  andrew  \n",
      "0       0.0    0.0    0.0     0.0    0.0   0.0       0.0     0.0  \n",
      "1       0.0    0.0    0.0     0.0    1.0   0.0       0.0     0.0  \n",
      "2       0.0    0.0    0.0     0.0    0.0   1.0       0.0     0.0  \n",
      "3       0.0    0.0    0.0     0.0    0.0   1.0       0.0     0.0  \n",
      "4       0.0    0.0    0.0     0.0    1.0   0.0       0.0     0.0  \n",
      "..      ...    ...    ...     ...    ...   ...       ...     ...  \n",
      "80      0.0    0.0    0.0     0.0    0.0   0.0       0.0     1.0  \n",
      "81      0.0    0.0    0.0     0.0    1.0   0.0       0.0     0.0  \n",
      "82      0.0    0.0    0.0     0.0    1.0   0.0       0.0     0.0  \n",
      "83      0.0    0.0    0.0     0.0    0.0   0.0       0.0     0.0  \n",
      "84      0.0    0.0    0.0     0.0    0.0   0.0       0.0     0.0  \n",
      "\n",
      "[85 rows x 17 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   student          85 non-null     float64\n",
      " 1   root             85 non-null     float64\n",
      " 2   piglet           85 non-null     float64\n",
      " 3   rainbow          85 non-null     float64\n",
      " 4   abc123           85 non-null     float64\n",
      " 5   passwd           85 non-null     float64\n",
      " 6   Hockey           85 non-null     float64\n",
      " 7   Mickey           85 non-null     float64\n",
      " 8   qwerty           85 non-null     float64\n",
      " 9   mustang          85 non-null     float64\n",
      " 10  admin            85 non-null     float64\n",
      " 11  money            85 non-null     float64\n",
      " 12  Justin           85 non-null     float64\n",
      " 13  david            85 non-null     float64\n",
      " 14  fred             85 non-null     float64\n",
      " 15  sunshine         85 non-null     float64\n",
      " 16  andrew           85 non-null     float64\n",
      " 17  is_private       85 non-null     int64  \n",
      " 18  is_failure       85 non-null     int64  \n",
      " 19  is_root          85 non-null     int64  \n",
      " 20  is_valid         85 non-null     int64  \n",
      " 21  not_valid_count  85 non-null     float64\n",
      " 22  ip_failure       85 non-null     float64\n",
      " 23  ip_success       85 non-null     float64\n",
      " 24  no_failure       85 non-null     float64\n",
      " 25  first            85 non-null     int64  \n",
      " 26  td               85 non-null     float64\n",
      " 27  ts               85 non-null     float64\n",
      "dtypes: float64(23), int64(5)\n",
      "memory usage: 18.7 KB\n",
      "None\n",
      "    student  root  piglet  rainbow  abc123  passwd  Hockey  Mickey  qwerty  \\\n",
      "0       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     1.0   \n",
      "1       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4       0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "..      ...   ...     ...      ...     ...     ...     ...     ...     ...   \n",
      "80      0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "81      0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "82      0.0   0.0     0.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "83      0.0   0.0     1.0      0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "84      0.0   0.0     0.0      0.0     0.0     0.0     0.0     1.0     0.0   \n",
      "\n",
      "    mustang  ...  is_failure  is_root  is_valid  not_valid_count  ip_failure  \\\n",
      "0       0.0  ...           1        0         0         0.900368    0.066760   \n",
      "1       0.0  ...           1        0         1        -0.474612   -0.325828   \n",
      "2       0.0  ...           1        0         0         5.418159    2.324143   \n",
      "3       0.0  ...           1        1         1        -0.474612   -0.227681   \n",
      "4       0.0  ...           1        0         1        -0.474612   -0.227681   \n",
      "..      ...  ...         ...      ...       ...              ...         ...   \n",
      "80      0.0  ...           1        0         1        -0.474612   -0.522123   \n",
      "81      0.0  ...           0        0         1        -0.474612   -0.620270   \n",
      "82      0.0  ...           1        0         1        -0.474612   -0.325828   \n",
      "83      0.0  ...           1        0         0         4.239605    1.735260   \n",
      "84      0.0  ...           1        0         0         2.078922    0.655642   \n",
      "\n",
      "    ip_success  no_failure  first        td        ts  \n",
      "0     1.502702    1.359731      0 -0.143809  0.539582  \n",
      "1     1.294960   -0.337420      0 -0.148574  0.524208  \n",
      "2    -0.782460    2.114020      0 -0.148988 -1.885637  \n",
      "3    -0.678589   -0.148848      0 -0.147987  0.522744  \n",
      "4    -0.782460   -0.148848      0 -0.148746 -1.952879  \n",
      "..         ...         ...    ...       ...       ...  \n",
      "80   -0.574718   -0.525992      0 -0.148228  0.530043  \n",
      "81   -0.678589   -0.714565      1 -0.149091 -1.947106  \n",
      "82   -0.782460   -0.243134      0 -0.148746 -1.952880  \n",
      "83   -0.782460    1.548303      0 -0.148988 -1.885638  \n",
      "84    0.671734   -0.054562      0 -0.148884  0.540258  \n",
      "\n",
      "[85 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "user_dummies = pd.get_dummies(ssh_encoded[\"user\"].astype(int), prefix=\"\", prefix_sep=\"\")\n",
    "def normalize_col(c):\n",
    "    s = str(c).strip()\n",
    "    return int(s) if s.isdigit() else s\n",
    "user_dummies = user_dummies.rename(columns=normalize_col)\n",
    "user_dummies = user_dummies.rename(columns=map_user)\n",
    "user_dummies = user_dummies.replace({True: 1.0, False: 0.0})\n",
    "print(user_dummies)\n",
    "data = pd.concat([user_dummies, ssh_encoded], axis=1)\n",
    "data = data.drop(columns=[\"user\"])\n",
    "cols_to_scale = [\"td\", \"ip_failure\", \"ip_success\", \"no_failure\", \"not_valid_count\"]\n",
    "data[cols_to_scale]= scalar_ip_no_td.transform(data[cols_to_scale])\n",
    "print(data.info())\n",
    "print(data)\n",
    "data.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test_norm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0268f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kamran', 'root', 'admins', 'phoenix', 'piglet', 'runner', 'sam', 'newpass', 'notused', 'internet', 'asshole', 'Maddock', 'computer', 'Mickey', 'qwerty', 'fiction', 'orange', 'tigger', 'wheeling', 'mustang', 'jennifer', 'money', 'chris', 'david', 'foobar', 'buster', 'harley', 'jordan', 'stupid', 'apple', 'fred', 'summer', 'andrew', 'osamac', 'gta', 'adminx', 'gtta', 'osamax', 'is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'ts']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 49 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   kamran           85 non-null     float64\n",
      " 1   root             85 non-null     float64\n",
      " 2   admins           85 non-null     float64\n",
      " 3   phoenix          85 non-null     float64\n",
      " 4   piglet           85 non-null     float64\n",
      " 5   runner           85 non-null     float64\n",
      " 6   sam              85 non-null     float64\n",
      " 7   newpass          85 non-null     float64\n",
      " 8   notused          85 non-null     float64\n",
      " 9   internet         85 non-null     float64\n",
      " 10  asshole          85 non-null     float64\n",
      " 11  Maddock          85 non-null     float64\n",
      " 12  computer         85 non-null     float64\n",
      " 13  Mickey           85 non-null     float64\n",
      " 14  qwerty           85 non-null     float64\n",
      " 15  fiction          85 non-null     float64\n",
      " 16  orange           85 non-null     float64\n",
      " 17  tigger           85 non-null     float64\n",
      " 18  wheeling         85 non-null     float64\n",
      " 19  mustang          85 non-null     float64\n",
      " 20  jennifer         85 non-null     float64\n",
      " 21  money            85 non-null     float64\n",
      " 22  chris            85 non-null     float64\n",
      " 23  david            85 non-null     float64\n",
      " 24  foobar           85 non-null     float64\n",
      " 25  buster           85 non-null     float64\n",
      " 26  harley           85 non-null     float64\n",
      " 27  jordan           85 non-null     float64\n",
      " 28  stupid           85 non-null     float64\n",
      " 29  apple            85 non-null     float64\n",
      " 30  fred             85 non-null     float64\n",
      " 31  summer           85 non-null     float64\n",
      " 32  andrew           85 non-null     float64\n",
      " 33  osamac           85 non-null     float64\n",
      " 34  gta              85 non-null     float64\n",
      " 35  adminx           85 non-null     float64\n",
      " 36  gtta             85 non-null     float64\n",
      " 37  osamax           85 non-null     float64\n",
      " 38  is_private       85 non-null     float64\n",
      " 39  is_failure       85 non-null     float64\n",
      " 40  is_root          85 non-null     float64\n",
      " 41  is_valid         85 non-null     float64\n",
      " 42  not_valid_count  85 non-null     float64\n",
      " 43  ip_failure       85 non-null     float64\n",
      " 44  ip_success       85 non-null     float64\n",
      " 45  no_failure       85 non-null     float64\n",
      " 46  first            85 non-null     float64\n",
      " 47  td               85 non-null     float64\n",
      " 48  ts               85 non-null     float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 32.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_data_normalized.csv\")\n",
    "expected_cols = original_data.columns.tolist()\n",
    "drop_val = [\"target\", \"source\"]\n",
    "for col in drop_val:\n",
    "    if col in expected_cols:\n",
    "        expected_cols.remove(col)\n",
    "print(expected_cols)\n",
    "data = data.reindex(columns=expected_cols, fill_value=0.0)\n",
    "data = data.astype(\"float64\")\n",
    "print(data.info())\n",
    "data.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test_norm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c85d0",
   "metadata": {},
   "source": [
    "### KMeans + centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    \n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_centroids_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a8f0b",
   "metadata": {},
   "source": [
    "### KMeans + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd840478",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_NN_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c21f6c",
   "metadata": {},
   "source": [
    "### KMeans + cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e2a8b",
   "metadata": {},
   "source": [
    "### K-means + cosinus + Mahalanobis distance (reduced to linear form, as it allows a simpler comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_MAN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5  #wazenie\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)   \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a47f7c",
   "metadata": {},
   "source": [
    "### HDBSCAN + Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e495470",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "labels_={}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels_[name] = len(unique_lables[unique_lables >=0])\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    \n",
    "    print(f\"{name}: {labels_[name]}\")\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "\n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_HDBSCAN_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_[\"smote\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42416291",
   "metadata": {},
   "source": [
    "### HDBSCAN + cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    print(labels)\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # policz dystanse do centroidu\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "       \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b791",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fcf38",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # policz dystanse do centroidu\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdc5d0",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_COS_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KMEANS_HDBSCAN_COS_NN_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1124158",
   "metadata": {},
   "source": [
    "#### -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2), init=\"k-means++\")\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    print(X_minority.shape)\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_COS_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(centroids_rows_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    print(df_majority.shape)  \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "    print(df_miniority.shape)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    print(df_[name].dtypes)\n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Num duplicates: {df_[name].duplicated().sum()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_SWAP_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(centroids_rows_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_SWAP_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "        \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(len(centroids_)):\n",
    "        \n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_HDBSCAN_DIST_[name].values(), ignore_index=True)\n",
    "        print(results_[name])\n",
    "        print(\"ilosc duplikaotow\",results_[name].duplicated().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
