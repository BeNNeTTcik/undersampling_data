{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e131a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import gower\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "#from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f499689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    151\n",
      "1     46\n",
      "Name: count, dtype: int64\n",
      "197\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\original_data.csv\")    \n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote_data.csv\")\n",
    "GAN_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN_data.csv\")\n",
    "borderline_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline_data.csv\")\n",
    "smote2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote3_data.csv\")\n",
    "GAN2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN3_data.csv\")    \n",
    "borderline2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline3_data.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\y_test.csv\")\n",
    "\n",
    "# load scalar and encoder map \n",
    "with open(\"D:\\\\ml\\\\undersampling_data\\\\models\\\\scaler_ip_no_td.pkl\", \"rb\") as f:\n",
    "    scalar_ip_no_td = pickle.load(f)\n",
    "with open(\"D:\\\\ml\\\\undersampling_data\\\\models\\\\map_user.pkl\", \"rb\") as f:\n",
    "    reverse_user_map = pickle.load(f)\n",
    "\n",
    "#Before undersampling\n",
    "print(original_data[\"target\"].value_counts())\n",
    "count1=original_data[\"target\"].value_counts().sum()\n",
    "print(count1)\n",
    "count2=abs((original_data['target']==0).sum() - (original_data['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")\n",
    "\n",
    "data_oririnal = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\SSH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b35c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed data\n",
    "\n",
    "\n",
    "#data with one oversampling method and original data e.g.(smote+original)\n",
    "smote_data = pd.concat([smote_data, smote2_data], axis=0, ignore_index=True)\n",
    "smote_data = smote_data.reset_index(drop=True)\n",
    "borderline_data = pd.concat([borderline_data, borderline2_data], axis=0, ignore_index=True)\n",
    "borderline_data = borderline_data.reset_index(drop=True)\n",
    "GAN_data = pd.concat([GAN_data, GAN2_data], axis=0, ignore_index=True)\n",
    "GAN_data = GAN_data.reset_index(drop=True)\n",
    "\n",
    "mix_data = pd.concat([GAN_data, smote_data, borderline_data], axis=0, ignore_index=True)         \n",
    "mix_data = mix_data.reset_index(drop=True)\n",
    "\n",
    "#convert data types to float64\n",
    "int_cols = mix_data.select_dtypes(include=[\"int\"]).columns\n",
    "mix_data[int_cols] = mix_data[int_cols].astype(\"float64\")\n",
    "int_cols = smote_data.select_dtypes(include=[\"int\"]).columns\n",
    "smote_data[int_cols] = smote_data[int_cols].astype(\"float64\")\n",
    "int_cols = borderline_data.select_dtypes(include=[\"int\"]).columns\n",
    "borderline_data[int_cols] = borderline_data[int_cols].astype(\"float64\")\n",
    "int_cols = GAN_data.select_dtypes(include=[\"int\"]).columns\n",
    "GAN_data[int_cols] = GAN_data[int_cols].astype(\"float64\")\n",
    "\n",
    "sum_all_data = pd.concat([smote_data, GAN_data, borderline_data, original_data], axis=0, ignore_index=True)\n",
    "sum_all_data = sum_all_data.drop_duplicates()\n",
    "\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_data.drop(columns=[\"target\", \"source\"]), mix_data[\"target\"]\n",
    "X_smote, y_smote = smote_data.drop(columns=[\"target\", \"source\"]), smote_data[\"target\"]\n",
    "X_GAN, y_GAN = GAN_data.drop(columns=[\"target\", \"source\"]), GAN_data[\"target\"]\n",
    "X_borderline, y_borderline = borderline_data.drop(columns=[\"target\", \"source\"]), borderline_data[\"target\"]\n",
    "\n",
    "#dodac standrazycje \n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_data\n",
    "compare[\"smote\"] = smote_data\n",
    "compare[\"GAN\"] = GAN_data\n",
    "compare[\"borderline\"] = borderline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300eef4",
   "metadata": {},
   "source": [
    "Dodatkowo mozna sprawdzic dzialanie \"init=\"k-means++\"\" na wyniki\n",
    "\n",
    "Stworzenie mixa mteryk i potem sprowadzenie do wartosci liniowej.\n",
    "\n",
    "- wyprobowanie gower distance na moich danych\n",
    "\n",
    "- zmiana danych (onehot endocer dla user oraz td zamiana na log1p(td))tak aby dalo sie wykorzystac metryki euclidean, cosinus similarity itd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab11a5",
   "metadata": {},
   "source": [
    "### Encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "unique_users = ssh_df[\"user\"].unique()\n",
    "print(unique_users)\n",
    "user_map = {user: idx for idx, user in enumerate(unique_users)}\n",
    "list(user_map.items())[:15]\n",
    "\n",
    "user_dummies = pd.get_dummies(original_data[\"user\"], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "ssh_encoded = pd.concat([ user_dummies,original_data], axis=1)\n",
    "\n",
    "reverse_user_map = {v: k for k, v in user_map.items()}\n",
    "print(reverse_user_map)\n",
    "ssh_encoded = ssh_encoded.rename(columns=reverse_user_map)\n",
    "print(ssh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_user_column(series, mapping):\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return series.map(mapping).astype(str)\n",
    "    else:\n",
    "        # spróbujmy rzutować na int\n",
    "        try:\n",
    "            as_int = series.astype(float).astype(int)\n",
    "            return as_int.map(mapping).astype(str)\n",
    "        except Exception:\n",
    "            # wygląda na to, że już są nazwy\n",
    "            return series.astype(str)\n",
    "        \n",
    "original_data[\"user\"] = map_user_column(original_data[\"user\"], reverse_user_map)\n",
    "user_dummies = pd.get_dummies(original_data[\"user\"], prefix=\"\", prefix_sep=\"\")\n",
    "ssh_encoded = pd.concat([ user_dummies,original_data], axis=1)\n",
    "ssh_encoded = ssh_encoded.replace({True: 1, False: 0})\n",
    "ssh_encoded = ssh_encoded.drop(columns=[\"user\"])\n",
    "\n",
    "scalar_ip_no_td = StandardScaler()\n",
    "cols_to_scale = [\"td\", \"ip_failure\", \"ip_success\", \"no_failure\", \"not_valid_count\"]\n",
    "ssh_encoded[cols_to_scale]= scalar_ip_no_td.fit_transform(ssh_encoded[cols_to_scale])\n",
    "ssh_encoded.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_normalized.csv\")\n",
    "with open(\"scaler_ip_no_td.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scalar_ip_no_td, f)\n",
    "with open(\"map_user.pkl\", \"wb\") as f:\n",
    "    pickle.dump(reverse_user_map, f)\n",
    "\n",
    "print(ssh_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdebd972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315, 14)\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print(smote_data.shape)\n",
    "print(data_oririnal[\"user\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bafe74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'ts', 'target', 'source']\n",
      "['user', 'is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'ts', 'target', 'source']\n",
      "{0: 'kamran', 1: 'student', 2: 'root', 3: 'admins', 4: 'phoenix', 5: 'piglet', 6: 'rainbow', 7: 'runner', 8: 'sam', 9: 'abc123', 10: 'passwd', 11: 'newpass', 12: 'notused', 13: 'Hockey', 14: 'internet', 15: 'asshole', 16: 'Maddock', 17: 'computer', 18: 'Mickey', 19: 'qwerty', 20: 'fiction', 21: 'orange', 22: 'tigger', 23: 'wheeling', 24: 'mustang', 25: 'admin', 26: 'jennifer', 27: 'money', 28: 'Justin', 29: 'chris', 30: 'david', 31: 'foobar', 32: 'buster', 33: 'harley', 34: 'jordan', 35: 'stupid', 36: 'apple', 37: 'fred', 38: 'summer', 39: 'sunshine', 40: 'andrew', 41: 'osamac', 42: 'gta', 43: 'adminx', 44: 'gtta', 45: 'osamax'}\n"
     ]
    }
   ],
   "source": [
    "ssh_encoded = mix_data\n",
    "#ssh_encoded = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "#ssh_encoded = ssh_encoded.drop(columns=[\"Unnamed: 0\"])\n",
    "cols = ssh_encoded.columns.tolist()\n",
    "print(cols)\n",
    "colums = original_data.columns.tolist()\n",
    "print(colums)\n",
    "print(reverse_user_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcfeb388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      kamran  student  root  admins  phoenix  piglet  rainbow  runner  sam  \\\n",
      "0        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "2        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "3        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  1.0   \n",
      "4        0.0      0.0   0.0     0.0      0.0     0.0      0.0     1.0  0.0   \n",
      "...      ...      ...   ...     ...      ...     ...      ...     ...  ...   \n",
      "1570     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1571     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1572     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1573     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1574     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "\n",
      "      abc123  ...  apple  fred  summer  sunshine  andrew  osamac  gta  adminx  \\\n",
      "0        1.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "1        0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "2        0.0  ...    0.0   0.0     0.0       0.0     1.0     0.0  0.0     0.0   \n",
      "3        0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "4        0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "...      ...  ...    ...   ...     ...       ...     ...     ...  ...     ...   \n",
      "1570     0.0  ...    0.0   1.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "1571     0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "1572     0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "1573     0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "1574     0.0  ...    0.0   0.0     0.0       0.0     0.0     0.0  0.0     0.0   \n",
      "\n",
      "      gtta  osamax  \n",
      "0      0.0     0.0  \n",
      "1      0.0     1.0  \n",
      "2      0.0     0.0  \n",
      "3      0.0     0.0  \n",
      "4      0.0     0.0  \n",
      "...    ...     ...  \n",
      "1570   0.0     0.0  \n",
      "1571   0.0     0.0  \n",
      "1572   0.0     0.0  \n",
      "1573   0.0     0.0  \n",
      "1574   0.0     0.0  \n",
      "\n",
      "[1575 rows x 46 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1575 entries, 0 to 1574\n",
      "Data columns (total 59 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   kamran           1575 non-null   float64\n",
      " 1   student          1575 non-null   float64\n",
      " 2   root             1575 non-null   float64\n",
      " 3   admins           1575 non-null   float64\n",
      " 4   phoenix          1575 non-null   float64\n",
      " 5   piglet           1575 non-null   float64\n",
      " 6   rainbow          1575 non-null   float64\n",
      " 7   runner           1575 non-null   float64\n",
      " 8   sam              1575 non-null   float64\n",
      " 9   abc123           1575 non-null   float64\n",
      " 10  passwd           1575 non-null   float64\n",
      " 11  newpass          1575 non-null   float64\n",
      " 12  notused          1575 non-null   float64\n",
      " 13  Hockey           1575 non-null   float64\n",
      " 14  internet         1575 non-null   float64\n",
      " 15  asshole          1575 non-null   float64\n",
      " 16  Maddock          1575 non-null   float64\n",
      " 17  computer         1575 non-null   float64\n",
      " 18  Mickey           1575 non-null   float64\n",
      " 19  qwerty           1575 non-null   float64\n",
      " 20  fiction          1575 non-null   float64\n",
      " 21  orange           1575 non-null   float64\n",
      " 22  tigger           1575 non-null   float64\n",
      " 23  wheeling         1575 non-null   float64\n",
      " 24  mustang          1575 non-null   float64\n",
      " 25  admin            1575 non-null   float64\n",
      " 26  jennifer         1575 non-null   float64\n",
      " 27  money            1575 non-null   float64\n",
      " 28  Justin           1575 non-null   float64\n",
      " 29  chris            1575 non-null   float64\n",
      " 30  david            1575 non-null   float64\n",
      " 31  foobar           1575 non-null   float64\n",
      " 32  buster           1575 non-null   float64\n",
      " 33  harley           1575 non-null   float64\n",
      " 34  jordan           1575 non-null   float64\n",
      " 35  stupid           1575 non-null   float64\n",
      " 36  apple            1575 non-null   float64\n",
      " 37  fred             1575 non-null   float64\n",
      " 38  summer           1575 non-null   float64\n",
      " 39  sunshine         1575 non-null   float64\n",
      " 40  andrew           1575 non-null   float64\n",
      " 41  osamac           1575 non-null   float64\n",
      " 42  gta              1575 non-null   float64\n",
      " 43  adminx           1575 non-null   float64\n",
      " 44  gtta             1575 non-null   float64\n",
      " 45  osamax           1575 non-null   float64\n",
      " 46  is_private       1575 non-null   float64\n",
      " 47  is_failure       1575 non-null   float64\n",
      " 48  is_root          1575 non-null   float64\n",
      " 49  is_valid         1575 non-null   float64\n",
      " 50  not_valid_count  1575 non-null   float64\n",
      " 51  ip_failure       1575 non-null   float64\n",
      " 52  ip_success       1575 non-null   float64\n",
      " 53  no_failure       1575 non-null   float64\n",
      " 54  first            1575 non-null   float64\n",
      " 55  td               1575 non-null   float64\n",
      " 56  ts               1575 non-null   float64\n",
      " 57  target           1575 non-null   float64\n",
      " 58  source           1575 non-null   object \n",
      "dtypes: float64(58), object(1)\n",
      "memory usage: 726.1+ KB\n",
      "None\n",
      "      kamran  student  root  admins  phoenix  piglet  rainbow  runner  sam  \\\n",
      "0        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "2        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "3        0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  1.0   \n",
      "4        0.0      0.0   0.0     0.0      0.0     0.0      0.0     1.0  0.0   \n",
      "...      ...      ...   ...     ...      ...     ...      ...     ...  ...   \n",
      "1570     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1571     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1572     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1573     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "1574     0.0      0.0   0.0     0.0      0.0     0.0      0.0     0.0  0.0   \n",
      "\n",
      "      abc123  ...  is_valid  not_valid_count  ip_failure  ip_success  \\\n",
      "0        1.0  ...       1.0         0.114665    2.716731   -0.782460   \n",
      "1        0.0  ...       0.0         1.882496    1.833407   -0.574718   \n",
      "2        0.0  ...       1.0         2.471773    2.422290   -0.678589   \n",
      "3        0.0  ...       1.0         0.900368    0.361201   -0.574718   \n",
      "4        0.0  ...       0.0        -0.278187    0.753790   -0.574718   \n",
      "...      ...  ...       ...              ...         ...         ...   \n",
      "1570     0.0  ...       1.0        -0.474612   -0.129534   -0.782460   \n",
      "1571     0.0  ...       0.0        -0.474612   -0.031387   -0.782460   \n",
      "1572     0.0  ...       1.0        -0.474612    0.066760   -0.782460   \n",
      "1573     0.0  ...       1.0        -0.474612    0.164907   -0.782460   \n",
      "1574     0.0  ...       0.0        -0.081761    0.361201   -0.574718   \n",
      "\n",
      "      no_failure  first        td        ts  target            source  \n",
      "0       0.699728    0.0 -0.148643 -0.735893     1.0               gan  \n",
      "1       1.265445    0.0 -0.148746 -0.735894     1.0               gan  \n",
      "2       2.679737    0.0 -0.148988 -0.735893     1.0               gan  \n",
      "3       1.076872    0.0 -0.148988 -0.735898     1.0               gan  \n",
      "4       0.228297    0.0 -0.149091 -0.735896     1.0               gan  \n",
      "...          ...    ...       ...       ...     ...               ...  \n",
      "1570   -0.054562    0.0 -0.149057  0.525504     1.0  borderline smote  \n",
      "1571    0.228297    0.0 -0.147987  0.523196     1.0  borderline smote  \n",
      "1572    0.322583    0.0 -0.148332 -1.474670     1.0  borderline smote  \n",
      "1573    0.511155    0.0 -0.148539 -1.929219     1.0  borderline smote  \n",
      "1574    0.039725    0.0 -0.148746  0.511550     1.0  borderline smote  \n",
      "\n",
      "[1575 rows x 59 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_3516\\2918200787.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  user_dummies = user_dummies.replace({True: 1.0, False: 0.0})\n"
     ]
    }
   ],
   "source": [
    "user_dummies = pd.get_dummies(ssh_encoded[\"user\"].astype(int), prefix=\"\", prefix_sep=\"\")\n",
    "def normalize_col(c):\n",
    "    s = str(c).strip()\n",
    "    return int(s) if s.isdigit() else s\n",
    "user_dummies = user_dummies.rename(columns=normalize_col)\n",
    "\n",
    "user_dummies = user_dummies.reindex(columns=reverse_user_map, fill_value=0.0)\n",
    "user_dummies = user_dummies.rename(columns=reverse_user_map)\n",
    "user_dummies = user_dummies.replace({True: 1.0, False: 0.0})\n",
    "print(user_dummies)\n",
    "data = pd.concat([user_dummies, ssh_encoded], axis=1)\n",
    "data = data.drop(columns=[\"user\"])\n",
    "cols_to_scale = [\"td\", \"ip_failure\", \"ip_success\", \"no_failure\", \"not_valid_count\"]\n",
    "data[cols_to_scale]= scalar_ip_no_td.transform(data[cols_to_scale])\n",
    "cols_to_conv = [ \"is_private\",\"is_failure\",\"is_root\", \"is_valid\", \"first\" , \"target\"]\n",
    "for col in cols_to_conv:\n",
    "    data[col] = data[col].astype(\"float64\")\n",
    "print(data.info())\n",
    "print(data)\n",
    "data.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\mix_data_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0268f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kamran', 'root', 'admins', 'phoenix', 'piglet', 'runner', 'sam', 'newpass', 'notused', 'internet', 'asshole', 'Maddock', 'computer', 'Mickey', 'qwerty', 'fiction', 'orange', 'tigger', 'wheeling', 'mustang', 'jennifer', 'money', 'chris', 'david', 'foobar', 'buster', 'harley', 'jordan', 'stupid', 'apple', 'fred', 'summer', 'andrew', 'osamac', 'gta', 'adminx', 'gtta', 'osamax', 'is_private', 'is_failure', 'is_root', 'is_valid', 'not_valid_count', 'ip_failure', 'ip_success', 'no_failure', 'first', 'td', 'ts']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 49 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   kamran           85 non-null     float64\n",
      " 1   root             85 non-null     float64\n",
      " 2   admins           85 non-null     float64\n",
      " 3   phoenix          85 non-null     float64\n",
      " 4   piglet           85 non-null     float64\n",
      " 5   runner           85 non-null     float64\n",
      " 6   sam              85 non-null     float64\n",
      " 7   newpass          85 non-null     float64\n",
      " 8   notused          85 non-null     float64\n",
      " 9   internet         85 non-null     float64\n",
      " 10  asshole          85 non-null     float64\n",
      " 11  Maddock          85 non-null     float64\n",
      " 12  computer         85 non-null     float64\n",
      " 13  Mickey           85 non-null     float64\n",
      " 14  qwerty           85 non-null     float64\n",
      " 15  fiction          85 non-null     float64\n",
      " 16  orange           85 non-null     float64\n",
      " 17  tigger           85 non-null     float64\n",
      " 18  wheeling         85 non-null     float64\n",
      " 19  mustang          85 non-null     float64\n",
      " 20  jennifer         85 non-null     float64\n",
      " 21  money            85 non-null     float64\n",
      " 22  chris            85 non-null     float64\n",
      " 23  david            85 non-null     float64\n",
      " 24  foobar           85 non-null     float64\n",
      " 25  buster           85 non-null     float64\n",
      " 26  harley           85 non-null     float64\n",
      " 27  jordan           85 non-null     float64\n",
      " 28  stupid           85 non-null     float64\n",
      " 29  apple            85 non-null     float64\n",
      " 30  fred             85 non-null     float64\n",
      " 31  summer           85 non-null     float64\n",
      " 32  andrew           85 non-null     float64\n",
      " 33  osamac           85 non-null     float64\n",
      " 34  gta              85 non-null     float64\n",
      " 35  adminx           85 non-null     float64\n",
      " 36  gtta             85 non-null     float64\n",
      " 37  osamax           85 non-null     float64\n",
      " 38  is_private       85 non-null     float64\n",
      " 39  is_failure       85 non-null     float64\n",
      " 40  is_root          85 non-null     float64\n",
      " 41  is_valid         85 non-null     float64\n",
      " 42  not_valid_count  85 non-null     float64\n",
      " 43  ip_failure       85 non-null     float64\n",
      " 44  ip_success       85 non-null     float64\n",
      " 45  no_failure       85 non-null     float64\n",
      " 46  first            85 non-null     float64\n",
      " 47  td               85 non-null     float64\n",
      " 48  ts               85 non-null     float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 32.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_data_normalized.csv\")\n",
    "expected_cols = original_data.columns.tolist()\n",
    "drop_val = [\"target\", \"source\"]\n",
    "for col in drop_val:\n",
    "    if col in expected_cols:\n",
    "        expected_cols.remove(col)\n",
    "print(expected_cols)\n",
    "data = data.reindex(columns=expected_cols, fill_value=0.0)\n",
    "data = data.astype(\"float64\")\n",
    "print(data.info())\n",
    "data.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test_norm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c85d0",
   "metadata": {},
   "source": [
    "### KMeans + centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    \n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_centroids_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a8f0b",
   "metadata": {},
   "source": [
    "### KMeans + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd840478",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_NN_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c21f6c",
   "metadata": {},
   "source": [
    "### KMeans + cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e2a8b",
   "metadata": {},
   "source": [
    "### K-means + cosinus + Mahalanobis distance (reduced to linear form, as it allows a simpler comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)(count2))\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_MAN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5  #wazenie\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)   \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a47f7c",
   "metadata": {},
   "source": [
    "### HDBSCAN + Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e495470",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "labels_={}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels_[name] = len(unique_lables[unique_lables >=0])\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    \n",
    "    print(f\"{name}: {labels_[name]}\")\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "\n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_HDBSCAN_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_[\"smote\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42416291",
   "metadata": {},
   "source": [
    "### HDBSCAN + cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    print(labels)\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # policz dystanse do centroidu\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "       \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0b791",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    #df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fcf38",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # policz dystanse do centroidu\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdc5d0",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=labels_[name])\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_COS_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    print(f\"{name}\")\n",
    "    print(centroids_)\n",
    "    #results_KM_SWAP_ = {}\n",
    "    target = count2\n",
    "    \n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(cluster_data_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KMEANS_HDBSCAN_COS_NN_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True) \n",
    "    print(df_[name]) \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_HDBSCAN_COS_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1124158",
   "metadata": {},
   "source": [
    "#### -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2), init=\"k-means++\")\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    print(X_minority.shape)\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_COS_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(centroids_rows_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    print(df_majority.shape)  \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "    print(df_miniority.shape)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    print(df_[name].dtypes)\n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Num duplicates: {df_[name].duplicated().sum()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_SWAP_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(centroids_rows_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_SWAP_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "        \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(len(centroids_)):\n",
    "        \n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_HDBSCAN_DIST_[name].values(), ignore_index=True)\n",
    "        print(results_[name])\n",
    "        print(\"ilosc duplikaotow\",results_[name].duplicated().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
