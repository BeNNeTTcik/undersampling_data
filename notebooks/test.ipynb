{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e131a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "#from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f499689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    151\n",
      "1     46\n",
      "Name: count, dtype: int64\n",
      "197\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\original_data.csv\")    \n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote_data.csv\")\n",
    "GAN_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN_data.csv\")\n",
    "borderline_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline_data.csv\")\n",
    "smote2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote3_data.csv\")\n",
    "GAN2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN3_data.csv\")    \n",
    "borderline2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline3_data.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\y_test.csv\")\n",
    "\n",
    "#Before undersampling\n",
    "print(original_data[\"target\"].value_counts())\n",
    "count1=original_data[\"target\"].value_counts().sum()\n",
    "print(count1)\n",
    "count2=abs((original_data['target']==0).sum() - (original_data['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b35c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed data\n",
    "mix_data = pd.concat([GAN_data, smote_data, borderline_data], axis=0, ignore_index=True)         \n",
    "mix_data = mix_data.reset_index(drop=True)\n",
    "\n",
    "#data with one oversampling method and original data e.g.(smote+original)\n",
    "smote_data = pd.concat([smote_data, smote2_data], axis=0, ignore_index=True)\n",
    "smote_data = smote_data.reset_index(drop=True)\n",
    "borderline_data = pd.concat([borderline_data, borderline2_data], axis=0, ignore_index=True)\n",
    "borderline_data = borderline_data.reset_index(drop=True)\n",
    "GAN_data = pd.concat([GAN_data, GAN2_data], axis=0, ignore_index=True)\n",
    "GAN_data = GAN_data.reset_index(drop=True)\n",
    "\n",
    "#convert data types to float64\n",
    "int_cols = mix_data.select_dtypes(include=[\"int\"]).columns\n",
    "mix_data[int_cols] = mix_data[int_cols].astype(\"float64\")\n",
    "int_cols = smote_data.select_dtypes(include=[\"int\"]).columns\n",
    "smote_data[int_cols] = smote_data[int_cols].astype(\"float64\")\n",
    "int_cols = borderline_data.select_dtypes(include=[\"int\"]).columns\n",
    "borderline_data[int_cols] = borderline_data[int_cols].astype(\"float64\")\n",
    "int_cols = GAN_data.select_dtypes(include=[\"int\"]).columns\n",
    "GAN_data[int_cols] = GAN_data[int_cols].astype(\"float64\")\n",
    "\n",
    "sum_all_data = pd.concat([smote_data, GAN_data, borderline_data, original_data], axis=0, ignore_index=True)\n",
    "sum_all_data = sum_all_data.drop_duplicates()\n",
    "\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_data.drop(columns=[\"target\", \"source\"]), mix_data[\"target\"]\n",
    "X_smote, y_smote = smote_data.drop(columns=[\"target\", \"source\"]), smote_data[\"target\"]\n",
    "X_GAN, y_GAN = GAN_data.drop(columns=[\"target\", \"source\"]), GAN_data[\"target\"]\n",
    "X_borderline, y_borderline = borderline_data.drop(columns=[\"target\", \"source\"]), borderline_data[\"target\"]\n",
    "\n",
    "#dodac standrazycje \n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_data\n",
    "compare[\"smote\"] = smote_data\n",
    "compare[\"GAN\"] = GAN_data\n",
    "compare[\"borderline\"] = borderline_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e495470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315, 12)\n",
      "borderline\n",
      "23\n",
      "     user  is_private  is_failure  is_root  is_valid  not_valid_count  \\\n",
      "0    30.0         1.0         1.0      0.0       0.0             13.0   \n",
      "1    37.0         1.0         1.0      0.0       0.0              0.0   \n",
      "2    35.0         1.0         1.0      0.0       0.0              0.0   \n",
      "3    25.0         1.0         1.0      0.0       0.0             11.0   \n",
      "4     8.0         1.0         1.0      0.0       0.0              0.0   \n",
      "..    ...         ...         ...      ...       ...              ...   \n",
      "297  30.0         1.0         1.0      1.0       1.0              0.0   \n",
      "298  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "299  24.0         1.0         0.0      0.0       1.0              0.0   \n",
      "300  18.0         1.0         0.0      0.0       1.0              0.0   \n",
      "301  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "\n",
      "     ip_failure  ip_success  no_failure  first      td        ts  target  \\\n",
      "0          13.0        13.0         8.0    0.0  1724.0  0.538912     1.0   \n",
      "1           5.0         0.0        13.0    0.0   717.0  0.518009     1.0   \n",
      "2           6.0         1.0         8.0    0.0  1208.0  0.512648     1.0   \n",
      "3          12.0        11.0         8.0    0.0  1491.0  0.537599     1.0   \n",
      "4           3.0         0.0        12.0    0.0    41.0  0.412306     1.0   \n",
      "..          ...         ...         ...    ...     ...       ...     ...   \n",
      "297         3.0         2.0         4.0    0.0     1.0  0.511124     0.0   \n",
      "298         0.0         4.0         0.0    0.0    26.0  0.512363     0.0   \n",
      "299         0.0         9.0         0.0    0.0     9.0  0.538242     0.0   \n",
      "300         0.0        33.0         0.0    0.0     8.0  0.540250     0.0   \n",
      "301         0.0        12.0         0.0    0.0    33.0  0.511790     0.0   \n",
      "\n",
      "               source  \n",
      "0    borderline smote  \n",
      "1    borderline smote  \n",
      "2    borderline smote  \n",
      "3    borderline smote  \n",
      "4    borderline smote  \n",
      "..                ...  \n",
      "297          original  \n",
      "298          original  \n",
      "299          original  \n",
      "300          original  \n",
      "301          original  \n",
      "\n",
      "[302 rows x 14 columns]\n",
      "0\n",
      "(315, 12)\n",
      "smote\n",
      "13\n",
      "     user  is_private  is_failure  is_root  is_valid  not_valid_count  \\\n",
      "0    27.0         1.0         1.0      0.0       0.0             12.0   \n",
      "1    41.0         1.0         1.0      0.0       0.0              9.0   \n",
      "2    16.0         1.0         1.0      0.0       0.0              0.0   \n",
      "3    32.0         0.0         1.0      1.0       1.0              0.0   \n",
      "4     7.0         1.0         1.0      0.0       0.0              9.0   \n",
      "..    ...         ...         ...      ...       ...              ...   \n",
      "297  30.0         1.0         1.0      1.0       1.0              0.0   \n",
      "298  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "299  24.0         1.0         0.0      0.0       1.0              0.0   \n",
      "300  18.0         1.0         0.0      0.0       1.0              0.0   \n",
      "301  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "\n",
      "     ip_failure  ip_success  no_failure  first    td        ts  target  \\\n",
      "0          43.0         0.0        43.0    0.0   6.0 -1.885632     1.0   \n",
      "1          40.0         0.0        40.0    0.0   3.0 -1.885634     1.0   \n",
      "2           4.0         1.0         7.0    0.0  15.0  0.512806     1.0   \n",
      "3           8.0         1.0        12.0    0.0   8.0  0.520657     1.0   \n",
      "4          40.0         0.0        40.0    0.0   3.0 -1.885634     1.0   \n",
      "..          ...         ...         ...    ...   ...       ...     ...   \n",
      "297         3.0         2.0         4.0    0.0   1.0  0.511124     0.0   \n",
      "298         0.0         4.0         0.0    0.0  26.0  0.512363     0.0   \n",
      "299         0.0         9.0         0.0    0.0   9.0  0.538242     0.0   \n",
      "300         0.0        33.0         0.0    0.0   8.0  0.540250     0.0   \n",
      "301         0.0        12.0         0.0    0.0  33.0  0.511790     0.0   \n",
      "\n",
      "       source  \n",
      "0       smote  \n",
      "1       smote  \n",
      "2       smote  \n",
      "3       smote  \n",
      "4       smote  \n",
      "..        ...  \n",
      "297  original  \n",
      "298  original  \n",
      "299  original  \n",
      "300  original  \n",
      "301  original  \n",
      "\n",
      "[302 rows x 14 columns]\n",
      "0\n",
      "(315, 12)\n",
      "GAN\n",
      "5\n",
      "     user  is_private  is_failure  is_root  is_valid  not_valid_count  \\\n",
      "0    14.0         1.0         1.0      0.0       1.0             13.0   \n",
      "1    24.0         1.0         1.0      0.0       1.0              7.0   \n",
      "2    18.0         0.0         1.0      0.0       1.0             21.0   \n",
      "3    17.0         1.0         1.0      0.0       1.0             15.0   \n",
      "4    16.0         1.0         1.0      1.0       1.0              4.0   \n",
      "..    ...         ...         ...      ...       ...              ...   \n",
      "297  30.0         1.0         1.0      1.0       1.0              0.0   \n",
      "298  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "299  24.0         1.0         0.0      0.0       1.0              0.0   \n",
      "300  18.0         1.0         0.0      0.0       1.0              0.0   \n",
      "301  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "\n",
      "     ip_failure  ip_success  no_failure  first      td        ts  target  \\\n",
      "0          19.0         1.0        19.0    0.0   285.0  1.274470     1.0   \n",
      "1          11.0         1.0        31.0    0.0  1351.0 -0.781320     1.0   \n",
      "2          31.0         0.0        31.0    0.0  1803.0  1.272936     1.0   \n",
      "3          22.0         0.0        18.0    0.0     8.0 -0.781325     1.0   \n",
      "4          22.0        14.0        34.0    0.0    87.0 -0.781323     1.0   \n",
      "..          ...         ...         ...    ...     ...       ...     ...   \n",
      "297         3.0         2.0         4.0    0.0     1.0  0.511124     0.0   \n",
      "298         0.0         4.0         0.0    0.0    26.0  0.512363     0.0   \n",
      "299         0.0         9.0         0.0    0.0     9.0  0.538242     0.0   \n",
      "300         0.0        33.0         0.0    0.0     8.0  0.540250     0.0   \n",
      "301         0.0        12.0         0.0    0.0    33.0  0.511790     0.0   \n",
      "\n",
      "       source  \n",
      "0         gan  \n",
      "1         gan  \n",
      "2         gan  \n",
      "3         gan  \n",
      "4         gan  \n",
      "..        ...  \n",
      "297  original  \n",
      "298  original  \n",
      "299  original  \n",
      "300  original  \n",
      "301  original  \n",
      "\n",
      "[302 rows x 14 columns]\n",
      "0\n",
      "(315, 12)\n",
      "mix\n",
      "7\n",
      "     user  is_private  is_failure  is_root  is_valid  not_valid_count  \\\n",
      "0     8.0         1.0         1.0      0.0       0.0             14.0   \n",
      "1     3.0         1.0         1.0      1.0       1.0              0.0   \n",
      "2    13.0         0.0         1.0      1.0       0.0              3.0   \n",
      "3     7.0         1.0         1.0      1.0       1.0              0.0   \n",
      "4    16.0         1.0         1.0      0.0       0.0              3.0   \n",
      "..    ...         ...         ...      ...       ...              ...   \n",
      "297  30.0         1.0         1.0      1.0       1.0              0.0   \n",
      "298  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "299  24.0         1.0         0.0      0.0       1.0              0.0   \n",
      "300  18.0         1.0         0.0      0.0       1.0              0.0   \n",
      "301  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "\n",
      "     ip_failure  ip_success  no_failure  first      td        ts  target  \\\n",
      "0          12.0         1.0        17.0    0.0  1351.0  1.352404     1.0   \n",
      "1          20.0        14.0        15.0    0.0  1803.0 -0.735900     1.0   \n",
      "2          11.0         0.0        10.0    0.0   285.0 -0.794195     1.0   \n",
      "3          10.0         0.0        12.0    0.0    87.0  1.365885     1.0   \n",
      "4          34.0         0.0        34.0    0.0     3.0 -1.885636     1.0   \n",
      "..          ...         ...         ...    ...     ...       ...     ...   \n",
      "297         3.0         2.0         4.0    0.0     1.0  0.511124     0.0   \n",
      "298         0.0         4.0         0.0    0.0    26.0  0.512363     0.0   \n",
      "299         0.0         9.0         0.0    0.0     9.0  0.538242     0.0   \n",
      "300         0.0        33.0         0.0    0.0     8.0  0.540250     0.0   \n",
      "301         0.0        12.0         0.0    0.0    33.0  0.511790     0.0   \n",
      "\n",
      "       source  \n",
      "0         gan  \n",
      "1         gan  \n",
      "2         gan  \n",
      "3         gan  \n",
      "4       smote  \n",
      "..        ...  \n",
      "297  original  \n",
      "298  original  \n",
      "299  original  \n",
      "300  original  \n",
      "301  original  \n",
      "\n",
      "[302 rows x 14 columns]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "\n",
    "#rows_in_cluster = {}\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    #X_minority = X_train[y_train == 0]\n",
    "    print(X_majority.shape)\n",
    "    #print(X_minority.shape)\n",
    "    print(name)\n",
    "    \n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    print(labels)\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    #print(hdbscan_res.centroids_)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "    #print(centroids_hdbscan)\n",
    "\n",
    "    #centroids\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        #print(f\"Cluster {i}:\")\n",
    "        #print(cluster_data_[name][i])\n",
    "        \n",
    "        \n",
    "        target = count2\n",
    "        #print(f\"Target: {target}\")\n",
    "        \n",
    "        per_cluster_sorted = {}\n",
    "        #calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # policz dystanse do centroidu\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin wybór do target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # wskaźnik który „numer” brać teraz z klastra\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "\n",
    "    # Złóż wybrane rekordy większości (kolejność wg selekcji)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected   \n",
    "    #print(results_[name])\n",
    "    #print(\"ilosc duplikaotow\",results_[name].duplicated().sum())\n",
    "        \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_data], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "    \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    #columns_ = list(df_[name].columns.values)\n",
    "    #df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])\n",
    "    print(df_[name].duplicated().sum())\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f862a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user               float64\n",
      "is_private         float64\n",
      "is_failure         float64\n",
      "is_root            float64\n",
      "is_valid           float64\n",
      "not_valid_count    float64\n",
      "ip_failure         float64\n",
      "ip_success         float64\n",
      "no_failure         float64\n",
      "first              float64\n",
      "td                 float64\n",
      "ts                 float64\n",
      "target             float64\n",
      "source              object\n",
      "dtype: object\n",
      "      user  is_private  is_failure  is_root  is_valid  not_valid_count  \\\n",
      "0     20.0         1.0         1.0      0.0       0.0              5.0   \n",
      "1     10.0         1.0         1.0      0.0       0.0              1.0   \n",
      "2     39.0         1.0         1.0      0.0       0.0             12.0   \n",
      "3     12.0         1.0         1.0      0.0       0.0              4.0   \n",
      "4     32.0         0.0         1.0      0.0       1.0              0.0   \n",
      "...    ...         ...         ...      ...       ...              ...   \n",
      "1137  30.0         1.0         1.0      1.0       1.0              0.0   \n",
      "1138  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "1139  24.0         1.0         0.0      0.0       1.0              0.0   \n",
      "1140  18.0         1.0         0.0      0.0       1.0              0.0   \n",
      "1141  30.0         1.0         0.0      0.0       1.0              0.0   \n",
      "\n",
      "      ip_failure  ip_success  no_failure  first    td        ts  target  \\\n",
      "0           36.0         0.0        36.0    0.0   3.0 -1.885635     1.0   \n",
      "1           32.0         0.0        32.0    0.0   3.0 -1.885636     1.0   \n",
      "2           43.0         0.0        43.0    0.0   8.0 -1.885633     1.0   \n",
      "3           35.0         0.0        35.0    0.0   3.0 -1.885635     1.0   \n",
      "4            8.0         0.0        12.0    0.0  22.0 -1.064140     1.0   \n",
      "...          ...         ...         ...    ...   ...       ...     ...   \n",
      "1137         3.0         2.0         4.0    0.0   1.0  0.511124     0.0   \n",
      "1138         0.0         4.0         0.0    0.0  26.0  0.512363     0.0   \n",
      "1139         0.0         9.0         0.0    0.0   9.0  0.538242     0.0   \n",
      "1140         0.0        33.0         0.0    0.0   8.0  0.540250     0.0   \n",
      "1141         0.0        12.0         0.0    0.0  33.0  0.511790     0.0   \n",
      "\n",
      "        source  \n",
      "0        smote  \n",
      "1        smote  \n",
      "2        smote  \n",
      "3        smote  \n",
      "4        smote  \n",
      "...        ...  \n",
      "1137  original  \n",
      "1138  original  \n",
      "1139  original  \n",
      "1140  original  \n",
      "1141  original  \n",
      "\n",
      "[1142 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sum_all_data.dtypes)\n",
    "print(df_[\"mix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1124158",
   "metadata": {},
   "source": [
    "#### -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2), init=\"k-means++\")\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    print(X_minority.shape)\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_COS_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):                #dla kazdego z centroidow\n",
    "        if (len(centroids_rows_[name][i])>1):       #sprawdzam czy jest wiecej niz jeden wiersz w klastrze\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    print(df_majority.shape)  \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "    print(df_miniority.shape)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    print(df_[name])\n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    print(df_[name].dtypes)\n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_COS_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Num duplicates: {df_[name].duplicated().sum()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_SWAP_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(centroids_rows_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_SWAP_[name].values(), ignore_index=True)    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "        \n",
    "     \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_NN_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(len(centroids_)):\n",
    "        \n",
    "        if (len(cluster_data_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_HDBSCAN_DIST_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_HDBSCAN_DIST_[name].values(), ignore_index=True)\n",
    "        print(results_[name])\n",
    "        print(\"ilosc duplikaotow\",results_[name].duplicated().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
