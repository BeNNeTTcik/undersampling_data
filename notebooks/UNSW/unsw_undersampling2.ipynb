{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_DYNAMIC\"] = \"FALSE\"\n",
    "from threadpoolctl import threadpool_limits, threadpool_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96887474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please note that you are missing the optional dependency: fugue. If you need to use this functionality it must be installed.\n",
      "Please note that you are missing the optional dependency: snowflake. If you need to use this functionality it must be installed.\n",
      "Please note that you are missing the optional dependency: spark. If you need to use this functionality it must be installed.\n",
      "Python 3.12 and above currently is not supported by Spark and Ray. Please note that some functionality will not work and currently is not supported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances, make_scorer, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, friedmanchisquare, wilcoxon\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from ctgan import CTGAN\n",
    "\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb201d77",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3120dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\CICUNSW_train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m gan_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mundersampling_data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcicunsw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moversampling\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGAN_data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m gan2_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mundersampling_data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcicunsw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moversampling\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGAN3_data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m original_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mundersampling_data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcicunsw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCICUNSW_train.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\CICUNSW_train.parquet'"
     ]
    }
   ],
   "source": [
    "borderline_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\borderline_data.parquet\") \n",
    "borderline2_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\borderline3_data.parquet\")\n",
    "smote_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\smote_data.parquet\")\n",
    "smote2_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\smote3_data.parquet\")\n",
    "gan_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\GAN_data.parquet\")\n",
    "gan2_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\GAN3_data.parquet\")\n",
    "\n",
    "original_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\CICUNSW_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacompy.Compare(original_df, borderline_df, join_columns='Label').report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dba7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "borderline_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df[\"Label\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smote_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smote_df[\"Label\"].value_counts())\n",
    "print(borderline_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b16979",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df = pd.concat([borderline_df, borderline2_df, smote_df, smote2_df, gan_df, gan2_df], ignore_index=True)\n",
    "smote_df = pd.concat([smote_df, smote2_df], ignore_index=True)\n",
    "gan_df = pd.concat([gan_df, gan2_df], ignore_index=True)\n",
    "borderline_df = pd.concat([borderline_df, borderline2_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d4387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    2415460\n",
      "1      62708\n",
      "Name: count, dtype: int64\n",
      "Before undersampling: 2352752\n",
      "After number of samples: 4830920\n"
     ]
    }
   ],
   "source": [
    "print(original_df[\"Label\"].value_counts())\n",
    "count1=original_df[\"Label\"].value_counts().sum()\n",
    "count2=abs((original_df['Label']==0).sum() - (original_df['Label']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_mix, y_mix = mix_df.drop(columns=[\"Label\", \"source\"]), mix_df[\"Label\"]\n",
    "X_smote, y_smote = smote_df.drop(columns=[\"Label\", \"source\"]), smote_df[\"Label\"]\n",
    "X_GAN, y_GAN = gan_df.drop(columns=[\"Label\", \"source\"]), gan_df[\"Label\"]\n",
    "X_borderline, y_borderline = borderline_df.drop(columns=[\"Label\", \"source\"]), borderline_df[\"Label\"]\n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_df\n",
    "compare[\"smote\"] = smote_df\n",
    "compare[\"GAN\"] = gan_df\n",
    "compare[\"borderline\"] = borderline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8977",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/cicunsw/reduced\")\n",
    "#base2 = Path(\"D:/ml/undersampling_data/models/kmeans&centroids2\")\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    with threadpool_limits(limits=16):\n",
    "    # Apply KMeans clustering\n",
    "        kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Create a DataFrame for centroids\n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"Label\") \n",
    "    \n",
    "    # Combine reduced majority class with original minority class\n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Add source column if not present\n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "    \n",
    "    # Combine with original minority class\n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44224cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    from cuml.cluster import KMeans as cuKMeans\n",
    "    GPU_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    GPU_AVAILABLE = False\n",
    "    GPU_IMPORT_ERROR = e\n",
    "\n",
    "# --- CPU fallback (jeśli chcesz, by kod działał także bez GPU) ---\n",
    "if not GPU_AVAILABLE:\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans as skKMeans\n",
    "else:\n",
    "    import pandas as pd  # użyjemy do I/O i finalnego reindexu\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # --- Wejście z dictów ---\n",
    "    X_train, y_train = data[name]      # pandas DataFrame/Series\n",
    "    compare_df = compare[name]         # nieużywane dalej\n",
    "\n",
    "    # --- Selekcja klasy większości: Label == 1 ---\n",
    "    #   Zakładam: y_train == 1 oznacza większość (jak w Twoim kodzie)\n",
    "    #   Jeśli odwrotnie, podmień warunek.\n",
    "    X_majority_pd = X_train[y_train == 1]\n",
    "\n",
    "    if GPU_AVAILABLE:\n",
    "        # ==== GPU ŚCIEŻKA ====\n",
    "        # Przenieś dane na GPU:\n",
    "        X_majority = cudf.from_pandas(X_majority_pd)\n",
    "\n",
    "        # KMeans na GPU:\n",
    "        KM = cuKMeans(\n",
    "            n_clusters=int(count2),\n",
    "            init=\"k-means++\",\n",
    "            max_iter=300,\n",
    "            random_state=42,\n",
    "            verbose=0,\n",
    "        )\n",
    "        kmeans = KM.fit(X_majority)\n",
    "\n",
    "        # Centroidy: cupy ndarray -> cuDF -> pandas\n",
    "        centers_cu = kmeans.cluster_centers_           # cupy.ndarray\n",
    "        X_majority_reduced_gdf = cudf.DataFrame(centers_cu, columns=X_train.columns)\n",
    "        X_majority_reduced = X_majority_reduced_gdf.to_pandas()\n",
    "\n",
    "    else:\n",
    "        # ==== CPU FALLBACK ====\n",
    "        KM = skKMeans(\n",
    "            n_clusters=int(count2),\n",
    "            init=\"k-means++\",\n",
    "            max_iter=300,\n",
    "            n_init=\"auto\",\n",
    "            random_state=42,\n",
    "            verbose=0,\n",
    "        )\n",
    "        kmeans = KM.fit(X_majority_pd)\n",
    "        X_majority_reduced = pd.DataFrame(\n",
    "            kmeans.cluster_centers_,\n",
    "            columns=X_train.columns\n",
    "        )\n",
    "\n",
    "    # Etykiety centroidów (większość)\n",
    "    y_majority_reduced = pd.Series([1] * int(count2), name=\"Label\")\n",
    "\n",
    "    # Zbuduj df dla większości (centroidy + Label)\n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Kolumna 'source'\n",
    "    if \"source\" not in df_majority.columns:\n",
    "        df_majority[\"source\"] = None\n",
    "    df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\"\n",
    "\n",
    "    # Dopasuj kolumny do original_df\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "\n",
    "    # Połącz z mniejszością (original_df)\n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Info i zapis\n",
    "    print(df_[name].info())\n",
    "\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File exists: {file_path}\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(f\"File saved: {file_path}\")\n",
    "\n",
    "# (opcjonalnie) komunikat jeśli szedłeś fallbackiem:\n",
    "if not GPU_AVAILABLE:\n",
    "    print(\n",
    "        \"\\n[WARN] Uruchomiono CPU fallback (cuDF/cuML niedostępne). \"\n",
    "        f\"Import error: {GPU_IMPORT_ERROR}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
