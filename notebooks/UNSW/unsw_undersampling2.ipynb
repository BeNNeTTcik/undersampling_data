{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_DYNAMIC\"] = \"FALSE\"\n",
    "from threadpoolctl import threadpool_limits, threadpool_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96887474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please note that you are missing the optional dependency: fugue. If you need to use this functionality it must be installed.\n",
      "Please note that you are missing the optional dependency: snowflake. If you need to use this functionality it must be installed.\n",
      "Please note that you are missing the optional dependency: spark. If you need to use this functionality it must be installed.\n",
      "Python 3.12 and above currently is not supported by Spark and Ray. Please note that some functionality will not work and currently is not supported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances, make_scorer, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, friedmanchisquare, wilcoxon\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from ctgan import CTGAN\n",
    "\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb201d77",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3120dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "borderline_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\borderline_data.parquet\") \n",
    "borderline2_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\borderline3_data.parquet\")\n",
    "smote_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\smote_data.parquet\")\n",
    "smote2_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\smote3_data.parquet\")\n",
    "gan_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\GAN_data.parquet\")\n",
    "gan2_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\oversampling\\\\GAN3_data.parquet\")\n",
    "\n",
    "original_df = pd.read_parquet(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\cicunsw\\\\CICUNSW_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacompy.Compare(original_df, borderline_df, join_columns='Label').report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dba7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "borderline_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df[\"Label\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smote_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smote_df[\"Label\"].value_counts())\n",
    "print(borderline_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b16979",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df = pd.concat([borderline_df, borderline2_df, smote_df, smote2_df, gan_df, gan2_df], ignore_index=True)\n",
    "smote_df = pd.concat([smote_df, smote2_df], ignore_index=True)\n",
    "gan_df = pd.concat([gan_df, gan2_df], ignore_index=True)\n",
    "borderline_df = pd.concat([borderline_df, borderline2_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1d4387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    2415460\n",
      "1      62708\n",
      "Name: count, dtype: int64\n",
      "Before undersampling: 2352752\n",
      "After number of samples: 4830920\n"
     ]
    }
   ],
   "source": [
    "print(original_df[\"Label\"].value_counts())\n",
    "count1=original_df[\"Label\"].value_counts().sum()\n",
    "count2=abs((original_df['Label']==0).sum() - (original_df['Label']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f0eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_mix, y_mix = mix_df.drop(columns=[\"Label\", \"source\"]), mix_df[\"Label\"]\n",
    "X_smote, y_smote = smote_df.drop(columns=[\"Label\", \"source\"]), smote_df[\"Label\"]\n",
    "X_GAN, y_GAN = gan_df.drop(columns=[\"Label\", \"source\"]), gan_df[\"Label\"]\n",
    "X_borderline, y_borderline = borderline_df.drop(columns=[\"Label\", \"source\"]), borderline_df[\"Label\"]\n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_df\n",
    "compare[\"smote\"] = smote_df\n",
    "compare[\"GAN\"] = gan_df\n",
    "compare[\"borderline\"] = borderline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8977",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/cicunsw/reduced\")\n",
    "#base2 = Path(\"D:/ml/undersampling_data/models/kmeans&centroids2\")\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    with threadpool_limits(limits=16):\n",
    "    # Apply KMeans clustering\n",
    "        kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Create a DataFrame for centroids\n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"Label\") \n",
    "    \n",
    "    # Combine reduced majority class with original minority class\n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Add source column if not present\n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "    \n",
    "    # Combine with original minority class\n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08a48ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 173\u001b[0m\n\u001b[0;32m    169\u001b[0m threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m                \u001b[38;5;66;03m# liczba wątków BLAS/MKL (zwykle = rdzenie fizyczne)\u001b[39;00m\n\u001b[0;32m    172\u001b[0m X_majority \u001b[38;5;241m=\u001b[39m borderline_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 173\u001b[0m centroids, inertia \u001b[38;5;241m=\u001b[39m kmeans_lloyd_streaming(\n\u001b[0;32m    174\u001b[0m     X_majority,\n\u001b[0;32m    175\u001b[0m     n_clusters\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    176\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    177\u001b[0m     n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    178\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m    179\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk,\n\u001b[0;32m    180\u001b[0m     init_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200_000\u001b[39m,\n\u001b[0;32m    181\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m    182\u001b[0m     blas_threads\u001b[38;5;241m=\u001b[39mthreads\n\u001b[0;32m    183\u001b[0m )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Centroidy → DataFrame + metadane, jak u Ciebie\u001b[39;00m\n\u001b[0;32m    186\u001b[0m X_cent \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(centroids, columns\u001b[38;5;241m=\u001b[39mX_majority\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "Cell \u001b[1;32mIn[6], line 93\u001b[0m, in \u001b[0;36mkmeans_lloyd_streaming\u001b[1;34m(X, n_clusters, max_iter, n_init, tol, chunk_size, init_sample, random_state, blas_threads)\u001b[0m\n\u001b[0;32m     90\u001b[0m best_C \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# inicjalizacja – próbka rezerwuarowa + k-means++\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m S \u001b[38;5;241m=\u001b[39m _reservoir_sample_rows(X, sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(init_sample, N), chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, rng\u001b[38;5;241m=\u001b[39mrng)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m threadpool_limits(limits\u001b[38;5;241m=\u001b[39mblas_threads):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m init_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_init):\n",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m, in \u001b[0;36m_reservoir_sample_rows\u001b[1;34m(X, sample_size, chunk_size, rng)\u001b[0m\n\u001b[0;32m     37\u001b[0m seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m sample_size:\n\u001b[1;32m---> 39\u001b[0m     buf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([buf, row[\u001b[38;5;28;01mNone\u001b[39;00m, :]])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     j \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m0\u001b[39m, seen)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\_core\\shape_base.py:291\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    290\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def _as_float32_c(X):\n",
    "    return np.asarray(X, dtype=np.float32, order=\"C\")\n",
    "\n",
    "def _batch_iter_df(Xdf: pd.DataFrame, chunk_size: int):\n",
    "    n = len(Xdf)\n",
    "    for s in range(0, n, chunk_size):\n",
    "        # bez zbędnych kopii\n",
    "        yield Xdf.iloc[s:s+chunk_size].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "def _batch_iter_np(Xnp: np.ndarray, chunk_size: int):\n",
    "    Xnp = _as_float32_c(Xnp)\n",
    "    n = Xnp.shape[0]\n",
    "    for s in range(0, n, chunk_size):\n",
    "        yield Xnp[s:s+chunk_size]\n",
    "\n",
    "def _iter_batches(X, chunk_size: int):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return _batch_iter_df(X, chunk_size)\n",
    "    return _batch_iter_np(X, chunk_size)\n",
    "\n",
    "def _reservoir_sample_rows(X, sample_size: int, chunk_size: int, rng):\n",
    "    \"\"\"Próbkowanie rezerwuarowe bez trzymania całości w RAM.\"\"\"\n",
    "    buf = None\n",
    "    seen = 0\n",
    "    for Xb in _iter_batches(X, chunk_size):\n",
    "        if buf is None:\n",
    "            take = min(sample_size, len(Xb))\n",
    "            buf = Xb[:take].copy(order=\"C\")\n",
    "            seen = take\n",
    "            # jeśli cały batch mniejszy niż sample -> dokładamy\n",
    "            if take < sample_size and len(Xb) > take:\n",
    "                extra = Xb[take: min(sample_size, len(Xb))].copy(order=\"C\")\n",
    "                buf = np.vstack([buf, extra])\n",
    "                seen = len(buf)\n",
    "        # reszta rezerwuaru\n",
    "        for row in Xb:\n",
    "            seen += 1\n",
    "            if buf.shape[0] < sample_size:\n",
    "                buf = np.vstack([buf, row[None, :]])\n",
    "            else:\n",
    "                j = rng.integers(0, seen)\n",
    "                if j < sample_size:\n",
    "                    buf[j] = row\n",
    "    return _as_float32_c(buf)\n",
    "\n",
    "def _kmeanspp_on_sample(S, k, rng):\n",
    "    \"\"\"k-means++ na próbce S (float32, (m,d)). Zwraca (k,d) float32.\"\"\"\n",
    "    m, d = S.shape\n",
    "    C = np.empty((k, d), dtype=np.float32)\n",
    "    C[0] = S[rng.integers(0, m)]\n",
    "    dist2 = np.full(m, np.inf, dtype=np.float32)\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        c = C[i-1]\n",
    "            # ||x-c||^2 = ||x||^2 + ||c||^2 - 2 x·c\n",
    "        x2 = np.sum(S*S, axis=1, dtype=np.float32)\n",
    "        c2 = np.sum(c*c, dtype=np.float32)\n",
    "        prod = S @ c\n",
    "        dist2 = np.minimum(dist2, x2 + c2 - 2.0*prod)  # min do najbliższego centrum\n",
    "        probs = dist2 / (dist2.sum() + 1e-12)\n",
    "        C[i] = S[rng.choice(m, p=probs)]\n",
    "    return C\n",
    "\n",
    "def kmeans_lloyd_streaming(\n",
    "    X,\n",
    "    n_clusters: int,\n",
    "    *,\n",
    "    max_iter: int = 100,\n",
    "    n_init: int = 5,\n",
    "    tol: float = 1e-4,\n",
    "    chunk_size: int = 100_000,\n",
    "    init_sample: int = 200_000,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Dokładny KMeans (Lloyd) liczony strumieniowo po chunkach.\n",
    "    X: np.ndarray (N,d) lub pd.DataFrame\n",
    "    Zwraca: (centroids: (k,d) float32, inertia: float)\n",
    "    \"\"\"\n",
    "    # wymiary\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        N, D = X.shape[0], X.shape[1]\n",
    "    else:\n",
    "        X = _as_float32_c(X)\n",
    "        N, D = X.shape\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    best_inertia = np.inf\n",
    "    best_C = None\n",
    "\n",
    "    # inicjalizacja – próbka rezerwuarowa + k-means++\n",
    "    S = _reservoir_sample_rows(X, sample_size=min(init_sample, N), chunk_size=chunk_size, rng=rng)\n",
    "\n",
    "    \n",
    "    for init_id in range(n_init):\n",
    "        C = _kmeanspp_on_sample(S, n_clusters, rng)\n",
    "\n",
    "        for it in range(max_iter):\n",
    "                # akumulatory (double dla stabilności sum)\n",
    "            sums = np.zeros_like(C, dtype=np.float64)\n",
    "            counts = np.zeros(n_clusters, dtype=np.int64)\n",
    "\n",
    "            C2 = np.sum(C*C, axis=1, dtype=np.float32)  # (k,)\n",
    "\n",
    "                # --- przypisania na chunkach ---\n",
    "            for Xb in _iter_batches(X, chunk_size):\n",
    "                    # Xb: (B,d) float32\n",
    "                x2 = np.sum(Xb*Xb, axis=1, dtype=np.float32)   # (B,)\n",
    "                prod = Xb @ C.T                                # (B,k)  BLAS\n",
    "                dist2 = x2[:, None] + C2[None, :] - 2.0 * prod\n",
    "                labels = np.argmin(dist2, axis=1)\n",
    "\n",
    "                    # akumulacja\n",
    "                for j in range(n_clusters):\n",
    "                    m = (labels == j)\n",
    "                    if m.any():\n",
    "                        counts[j] += int(m.sum())\n",
    "                        sums[j] += Xb[m].sum(axis=0, dtype=np.float64)\n",
    "\n",
    "                # aktualizacja centrów\n",
    "            newC = C.copy()\n",
    "            nz = counts > 0\n",
    "            newC[nz] = (sums[nz] / counts[nz][:, None]).astype(np.float32)\n",
    "\n",
    "                # puste klastry → losowo z próbki (stabilne)\n",
    "            if (~nz).any():\n",
    "                repl = _kmeanspp_on_sample(S, (~nz).sum(), rng)\n",
    "                newC[~nz] = repl\n",
    "\n",
    "            # kryterium zbieżności (L2 przesunięcia)\n",
    "            shift = np.linalg.norm((newC - C).astype(np.float64))\n",
    "            C = newC\n",
    "            if shift <= tol * max(1.0, np.linalg.norm(C.astype(np.float64))):\n",
    "                break\n",
    "\n",
    "            # policz inercję (sumę min dystansów^2) – też strumieniowo\n",
    "        C2 = np.sum(C*C, axis=1, dtype=np.float32)\n",
    "        inertia = 0.0\n",
    "        for Xb in _iter_batches(X, chunk_size):\n",
    "            x2 = np.sum(Xb*Xb, axis=1, dtype=np.float32)\n",
    "            prod = Xb @ C.T\n",
    "            dist2 = x2[:, None] + C2[None, :] - 2.0 * prod\n",
    "            inertia += float(np.min(dist2, axis=1).sum())\n",
    "\n",
    "        if inertia < best_inertia:\n",
    "            best_inertia = inertia\n",
    "            best_C = C.copy()\n",
    "\n",
    "    return best_C, best_inertia\n",
    "\n",
    "def assign_labels_streaming(X, centroids, *, chunk_size=100_000):\n",
    "    \"\"\"Przypisz etykiety do wszystkich próbek na bazie centroidów (też strumieniowo).\"\"\"\n",
    "    C = _as_float32_c(centroids)\n",
    "    C2 = np.sum(C*C, axis=1, dtype=np.float32)\n",
    "    labels_all = []\n",
    "    \n",
    "    for Xb in _iter_batches(X, chunk_size):\n",
    "        x2 = np.sum(Xb*Xb, axis=1, dtype=np.float32)\n",
    "        prod = Xb @ C.T\n",
    "        dist2 = x2[:, None] + C2[None, :] - 2.0 * prod\n",
    "        labels_all.append(np.argmin(dist2, axis=1))\n",
    "    return np.concatenate(labels_all, axis=0)\n",
    "\n",
    "\n",
    "# Załóżmy: X_majority — DataFrame/ndarray tylko z cechami (bez Label), w większości float32\n",
    "k = 100000                 # realny pułap dla 7.1M; większe wartości rosną bardzo mocno w czasie/RAM\n",
    "chunk = 100000             # dopasuj do RAM (zmniejsz gdy pamięć ~90%)\n",
    "\n",
    "\n",
    "X_majority = borderline_df.drop(columns=[\"Label\", \"source\"])\n",
    "centroids, inertia = kmeans_lloyd_streaming(\n",
    "    X_majority,\n",
    "    n_clusters=k,\n",
    "    max_iter=30,\n",
    "    n_init=3,\n",
    "    tol=1e-4,\n",
    "    chunk_size=chunk,\n",
    "    init_sample=200_000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Centroidy → DataFrame + metadane, jak u Ciebie\n",
    "X_cent = pd.DataFrame(centroids, columns=X_majority.columns)\n",
    "y_cent = pd.Series(np.ones(k, dtype=np.int8), name=\"Label\")\n",
    "df_majority = pd.concat([X_cent, y_cent], axis=1)\n",
    "df_majority[\"source\"] = \"centroid\"\n",
    "\n",
    "# (opcjonalnie) etykiety dla wszystkich próbek:\n",
    "# labels = assign_labels_streaming(X_majority, centroids, chunk_size=chunk, blas_threads=threads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
