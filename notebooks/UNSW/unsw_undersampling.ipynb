{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb58c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances, make_scorer, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, friedmanchisquare, wilcoxon\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e2b8b",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e883076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling data\n",
    "original_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\encoded_normalized\\\\original_data_normalized.csv\")\n",
    "mix_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\encoded_normalized\\\\mix_data_normalized.csv\")\n",
    "smote_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\encoded_normalized\\\\smote_data_normalized.csv\") \n",
    "gan_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\encoded_normalized\\\\gan_data_normalized.csv\")\n",
    "borderline_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\encoded_normalized\\\\borderline_data_normalized.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\test\\\\X_test_norm.csv\")\n",
    "\n",
    "#Before undersampling\n",
    "print(original_df[\"target\"].value_counts())\n",
    "count1=original_df[\"target\"].value_counts().sum()\n",
    "count2=abs((original_df['target']==0).sum() - (original_df['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum up all df \n",
    "sum_all_data = pd.concat([smote_df, gan_df, borderline_df, original_df], axis=0, ignore_index=True)\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_df.drop(columns=[\"target\", \"source\"]), mix_df[\"target\"]\n",
    "X_smote, y_smote = smote_df.drop(columns=[\"target\", \"source\"]), smote_df[\"target\"]\n",
    "X_GAN, y_GAN = gan_df.drop(columns=[\"target\", \"source\"]), gan_df[\"target\"]\n",
    "X_borderline, y_borderline = borderline_df.drop(columns=[\"target\", \"source\"]), borderline_df[\"target\"]\n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_df\n",
    "compare[\"smote\"] = smote_df\n",
    "compare[\"GAN\"] = gan_df\n",
    "compare[\"borderline\"] = borderline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee320ac3",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09318db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&centroids\")\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Create a DataFrame for centroids\n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    \n",
    "    # Combine reduced majority class with original minority class\n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Add source column if not present\n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "    \n",
    "    # Combine with original minority class\n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5dfa71",
   "metadata": {},
   "source": [
    "### KMeans + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4542b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original minority class\n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(cluster_data_[name][i])>1):     # If more than one sample in the cluster\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         # Colculate Euclidean distance\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]    # Select the nearest neighbor to the centroid\n",
    "            \n",
    "        else:           # If only one sample in the cluster\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_NN_[name].values(), ignore_index=True)    \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_nn.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa966ef",
   "metadata": {},
   "source": [
    "### KMeans + Cosinus Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83195f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original majority class\n",
    "    for i in range(len(centroids_)):                \n",
    "        if (len(cluster_data_[name][i])>1):       # If more than one sample in the cluster\n",
    "            dist_ = {}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         # Colculate Cosine similarity\n",
    "                \n",
    "            min_key = max(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]   # Select the nearest neighbor to the centroid\n",
    "            \n",
    "        else:       # If only one sample in the cluster\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)       \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ed5a8",
   "metadata": {},
   "source": [
    "### KMeans + cos + Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86592b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&cos&mal\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_MAN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5  #wazenie\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "        \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original majority class\n",
    "    for i in range(len(centroids_)):               \n",
    "        if (len(cluster_data_[name][i])>1):       # If more than one sample in the cluster\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)[0,0]          # using Cosine similarity\n",
    "                dist_[j] = distance.mahalanobis(centroid.flatten(), row.flatten(), np.linalg.pinv(np.cov(X_train.T)))  # using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)   \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_cos_mal.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d667d1c",
   "metadata": {},
   "source": [
    "### HDBSCAN + Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/hdbscan&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "labels_={}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply HDBSCAN clustering\n",
    "    hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    \n",
    "    # Save HDBSCAN model\n",
    "    file_path2 = base2 / f\"{name}_hdbscan_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(hdbscan, f)\n",
    "    \n",
    "    # Get labels and number of clusters\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels_[name] = len(unique_lables[unique_lables >=0])\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    print(f\"{name}: {labels_[name]}\")\n",
    "    \n",
    "    # Centroids from HDBSCAN\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "\n",
    "    # Add rows to cluster dictionary\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "\n",
    "        target = count2\n",
    "        \n",
    "        # Sorted neighbors dictionary\n",
    "        per_cluster_sorted = {}\n",
    "        \n",
    "        # Calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # Calculate Euclidean distances from centroid to all points in the cluster\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin chooice to target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # Which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:\n",
    "            break\n",
    "\n",
    "    # Choose selected records of majority class (in order of selection)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)\n",
    "    results_[name] = maj_selected   \n",
    "    \n",
    "    # Combine reduced majority class with original df    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "\n",
    "    print(df_[name].info())\n",
    "\n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_HDBSCAN_NN.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e177b",
   "metadata": {},
   "source": [
    "### HDBSCAN + Cosinus Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/hdbscan&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply HDBSCAN clustering\n",
    "    hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    \n",
    "    # Save HDBSCAN model\n",
    "    file_path2 = base2 / f\"{name}_hdbscan_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(hdbscan, f)\n",
    "        \n",
    "    # Get labels and number of clusters\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "\n",
    "    # Centroids from HDBSCAN + Add rows to cluster dictionary\n",
    "    for i in range(labels):\n",
    "        # Add rows to cluster dictionary\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        target = count2\n",
    "\n",
    "        # Sorted crucial indexes from clusters\n",
    "        per_cluster_sorted = {}\n",
    "        \n",
    "        # Calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # Calculate Cosine distances from centroid to all points in the cluster\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin chooice to target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                 # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed: \n",
    "            break\n",
    "\n",
    "    # Choose selected records of majority class (in order of selection)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)\n",
    "    results_[name] = maj_selected   \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_HDBSCAN_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d665fef",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e260bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&hdbscan&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "        \n",
    "    # Centroids from KMeans + Add rows to cluster dictionary\n",
    "    for i in range (labels_[name]):\n",
    "        # Add rows to cluster dictionary\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    target = count2\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # Calculate Euclidean distances from centroid to all points in the cluster\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indexes of X_majority in ascending order of distances\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed: \n",
    "            break\n",
    "    \n",
    "    # Choose selected records of majority class (in order of selection)   \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   \n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info()) \n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_kmeans&hdbscan_nn.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee984bb5",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&hdbscan&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples) \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids from KMeans + Add rows to cluster dictionary\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    target = count2\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # Calculate Cosine distances from centroid to all points in the cluster\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()              # indexes of X_majority in ascending order of distances\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    # Select selected records of majority class (in order of selection)    \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    print(df_[name].info()) \n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_kmeans&hdbscan_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe78987",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus + Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a54093",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&hdbscan&cos&mal\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_COS_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids from KMeans + Add rows to cluster dictionary\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "        \n",
    "    target = count2\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):                # for each centroid\n",
    "        if (len(cluster_data_[name][i])>1):        # check if more than one sample in the cluster\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {}                # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)[0,0]         #using Cosine similarity\n",
    "                dist_[j] = distance.mahalanobis(centroid.flatten(), row.flatten(), np.linalg.pinv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "            order = sorted(comb_score_, key=comb_score_.get, reverse=True)\n",
    "            per_cluster_sorted[i] = [cluster_data_[name][i].index[j] for j in order]\n",
    "                \n",
    "        else:\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "    results_[name] = pd.concat(results_KMEANS_HDBSCAN_COS_NN_[name].values(), ignore_index=True)\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted.get(i, [])\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  \n",
    "            break\n",
    "\n",
    "    # Choose selected records of majority class (in order of selection)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    print(df_[name].info()) \n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_kmeans&hdbscan_cos&mal.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
