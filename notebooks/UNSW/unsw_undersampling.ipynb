{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6a018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "\n",
    "for lib in threadpool_info():\n",
    "    print(lib['internal_api'], lib.get('num_threads'), lib.get('prefix', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb58c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please note that you are missing the optional dependency: fugue. If you need to use this functionality it must be installed.\n",
      "Please note that you are missing the optional dependency: snowflake. If you need to use this functionality it must be installed.\n",
      "Please note that you are missing the optional dependency: spark. If you need to use this functionality it must be installed.\n",
      "Python 3.12 and above currently is not supported by Spark and Ray. Please note that some functionality will not work and currently is not supported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances, make_scorer, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, friedmanchisquare, wilcoxon\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from ctgan import CTGAN\n",
    "\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e2b8b",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcd6ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m brddata1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mundersampling_data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124munsw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moversampling\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mborderline_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m brddata3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mundersampling_data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124munsw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moversampling\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mborderline3_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m GANdata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mundersampling_data\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124munsw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124moversampling\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGAN_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:334\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "brddata1 = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\borderline_data.csv\")\n",
    "brddata3 = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\borderline3_data.csv\")\n",
    "GANdata = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\GAN_data.csv\")\n",
    "GANdata3 = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\GAN3_data.csv\")\n",
    "smotedata1 = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\smote_data.csv\")\n",
    "smotedata3 = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\smote3_data.csv\")\n",
    "\n",
    "X_train = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\X_train.csv\")\n",
    "y_train = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e145c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df = pd.concat([brddata1, brddata3, GANdata, GANdata3, smotedata1, smotedata3], ignore_index=True)\n",
    "smote_df = pd.concat([smotedata1, smotedata3], ignore_index=True)\n",
    "borderline_df = pd.concat([brddata1, brddata3], ignore_index=True)\n",
    "GAN_df = pd.concat([GANdata, GANdata3], ignore_index=True)\n",
    "original_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "mix_df.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\mix_data.csv\", index=False)\n",
    "smote_df.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\smote_data.csv\", index=False)\n",
    "borderline_df.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\borderline_data.csv\", index=False)\n",
    "GAN_df.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\GAN_data.csv\", index=False)\n",
    "original_df.to_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\original_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e883076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    1370400\n",
      "1      61898\n",
      "Name: count, dtype: int64\n",
      "Before undersampling: 1308502\n",
      "After number of samples: 2740800\n"
     ]
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\original_data.csv\")\n",
    "#mix_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\mix_data.csv\")\n",
    "smote_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\smote_data.csv\") \n",
    "#gan_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\gan_data.csv\")\n",
    "borderline_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\syntheticdf\\\\borderline_data.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\X_test.csv\")\n",
    "\n",
    "#Before undersampling\n",
    "print(original_df[\"Label\"].value_counts())\n",
    "count1=original_df[\"Label\"].value_counts().sum()\n",
    "count2=abs((original_df['Label']==0).sum() - (original_df['Label']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04ebf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    3925506\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6add62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum up all df \n",
    "#sum_all_data = pd.concat([smote_df, gan_df, borderline_df, original_df], axis=0, ignore_index=True)\n",
    "\n",
    "#Split data\n",
    "#X_mix, y_mix = mix_df.drop(columns=[\"Label\", \"source\"]), mix_df[\"Label\"]\n",
    "X_smote, y_smote = smote_df.drop(columns=[\"Label\", \"source\"]), smote_df[\"Label\"]\n",
    "#X_GAN, y_GAN = gan_df.drop(columns=[\"Label\", \"source\"]), gan_df[\"Label\"]\n",
    "X_borderline, y_borderline = borderline_df.drop(columns=[\"Label\", \"source\"]), borderline_df[\"Label\"]\n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "#data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "#data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "#compare[\"mix\"] = mix_df\n",
    "compare[\"smote\"] = smote_df\n",
    "#compare[\"GAN\"] = gan_df\n",
    "compare[\"borderline\"] = borderline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee320ac3",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09318db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Nie można odnaleźć określonego pliku\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m KM \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mint\u001b[39m)(count2))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m threadpool_limits(limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Apply KMeans clustering\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KM\u001b[38;5;241m.\u001b[39mfit(X_majority)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for centroids\u001b[39;00m\n\u001b[0;32m     20\u001b[0m X_majority_reduced \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_, columns\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1499\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1495\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[0;32m   1498\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[1;32m-> 1499\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_centroids(\n\u001b[0;32m   1500\u001b[0m         X,\n\u001b[0;32m   1501\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[0;32m   1502\u001b[0m         init\u001b[38;5;241m=\u001b[39minit,\n\u001b[0;32m   1503\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m   1504\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1505\u001b[0m     )\n\u001b[0;32m   1506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m   1507\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1014\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[1;34m(self, X, x_squared_norms, init, random_state, sample_weight, init_size, n_centroids)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[init_indices]\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1014\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m _kmeans_plusplus(\n\u001b[0;32m   1015\u001b[0m         X,\n\u001b[0;32m   1016\u001b[0m         n_clusters,\n\u001b[0;32m   1017\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m   1018\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[0;32m   1019\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1022\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[0;32m   1023\u001b[0m         n_samples,\n\u001b[0;32m   1024\u001b[0m         size\u001b[38;5;241m=\u001b[39mn_clusters,\n\u001b[0;32m   1025\u001b[0m         replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1026\u001b[0m         p\u001b[38;5;241m=\u001b[39msample_weight \u001b[38;5;241m/\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum(),\n\u001b[0;32m   1027\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:256\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[1;34m(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    251\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m _euclidean_distances(\n\u001b[0;32m    252\u001b[0m     X[candidate_ids], X, Y_norm_squared\u001b[38;5;241m=\u001b[39mx_squared_norms, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    253\u001b[0m )\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m np\u001b[38;5;241m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates)\n\u001b[0;32m    257\u001b[0m candidates_pot \u001b[38;5;241m=\u001b[39m distance_to_candidates \u001b[38;5;241m@\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Decide which candidate is the best\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&centroids2\")\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    with threadpool_limits(limits=16):\n",
    "    # Apply KMeans clustering\n",
    "        kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Create a DataFrame for centroids\n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"Label\") \n",
    "    \n",
    "    # Combine reduced majority class with original minority class\n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Add source column if not present\n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "    \n",
    "    # Combine with original minority class\n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8372b16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Przetwarzanie: smote ===\n",
      "• Fitting IncrementalPCA...\n",
      "• Transforming data with PCA...\n",
      "• Running KMeans clustering...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m• Running KMeans clustering...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m threadpool_limits(limits\u001b[38;5;241m=\u001b[39mN_THREADS):\n\u001b[1;32m---> 55\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KM\u001b[38;5;241m.\u001b[39mfit(X_reduced)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# --- Odwrotna transformacja centroidów do oryginalnej przestrzeni ---\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m• Inverse-transforming centroids...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1499\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1495\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[0;32m   1498\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[1;32m-> 1499\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_centroids(\n\u001b[0;32m   1500\u001b[0m         X,\n\u001b[0;32m   1501\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[0;32m   1502\u001b[0m         init\u001b[38;5;241m=\u001b[39minit,\n\u001b[0;32m   1503\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m   1504\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1505\u001b[0m     )\n\u001b[0;32m   1506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m   1507\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1014\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[1;34m(self, X, x_squared_norms, init, random_state, sample_weight, init_size, n_centroids)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[init_indices]\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1014\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m _kmeans_plusplus(\n\u001b[0;32m   1015\u001b[0m         X,\n\u001b[0;32m   1016\u001b[0m         n_clusters,\n\u001b[0;32m   1017\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m   1018\u001b[0m         x_squared_norms\u001b[38;5;241m=\u001b[39mx_squared_norms,\n\u001b[0;32m   1019\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1022\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[0;32m   1023\u001b[0m         n_samples,\n\u001b[0;32m   1024\u001b[0m         size\u001b[38;5;241m=\u001b[39mn_clusters,\n\u001b[0;32m   1025\u001b[0m         replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1026\u001b[0m         p\u001b[38;5;241m=\u001b[39msample_weight \u001b[38;5;241m/\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum(),\n\u001b[0;32m   1027\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:256\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[1;34m(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    251\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m _euclidean_distances(\n\u001b[0;32m    252\u001b[0m     X[candidate_ids], X, Y_norm_squared\u001b[38;5;241m=\u001b[39mx_squared_norms, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    253\u001b[0m )\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m np\u001b[38;5;241m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates)\n\u001b[0;32m    257\u001b[0m candidates_pot \u001b[38;5;241m=\u001b[39m distance_to_candidates \u001b[38;5;241m@\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Decide which candidate is the best\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# ŚCIEŻKI\n",
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&centroids2\")\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "# ⚙️ PARAMETRY\n",
    "N_THREADS = 16            # lub 24 jeśli CPU i RAM dają radę\n",
    "pca_components = 50       # liczba wymiarów po redukcji\n",
    "batch_size = 8192         # rozmiar chunku do PCA\n",
    "max_iter = 100\n",
    "n_init = 3\n",
    "random_state = 42\n",
    "\n",
    "KM = KMeans(\n",
    "    n_clusters=int(count2),\n",
    "    algorithm=\"elkan\",      # szybszy przy Euklidesie\n",
    "    n_init=n_init,\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "for name in (data.keys() & compare.keys()):\n",
    "    print(f\"\\n=== Przetwarzanie: {name} ===\")\n",
    "\n",
    "    # --- przygotowanie danych ---\n",
    "    X_train, y_train = data[name]\n",
    "    mask = (y_train == 1)\n",
    "    X_majority = np.ascontiguousarray(X_train.loc[mask].to_numpy(np.float32, copy=False))\n",
    "\n",
    "    # --- PCA: dopasowanie i transformacja strumieniowo ---\n",
    "    print(\"• Fitting IncrementalPCA...\")\n",
    "    ipca = IncrementalPCA(n_components=pca_components, batch_size=batch_size)\n",
    "    for start in range(0, X_majority.shape[0], batch_size):\n",
    "        ipca.partial_fit(X_majority[start:start + batch_size])\n",
    "\n",
    "    print(\"• Transforming data with PCA...\")\n",
    "    X_reduced = np.empty((X_majority.shape[0], pca_components), dtype=np.float32)\n",
    "    for start in range(0, X_majority.shape[0], batch_size):\n",
    "        X_reduced[start:start + batch_size] = ipca.transform(\n",
    "            X_majority[start:start + batch_size]\n",
    "        ).astype(np.float32, copy=False)\n",
    "\n",
    "    # --- KMeans w zredukowanej przestrzeni ---\n",
    "    print(\"• Running KMeans clustering...\")\n",
    "    with threadpool_limits(limits=N_THREADS):\n",
    "        kmeans = KM.fit(X_reduced)\n",
    "\n",
    "    # --- Odwrotna transformacja centroidów do oryginalnej przestrzeni ---\n",
    "    print(\"• Inverse-transforming centroids...\")\n",
    "    centers = ipca.inverse_transform(kmeans.cluster_centers_).astype(np.float32, copy=False)\n",
    "\n",
    "    # --- Zbudowanie DataFrame wynikowego ---\n",
    "    df_majority = pd.DataFrame(centers, columns=X_train.columns, copy=False)\n",
    "    df_majority[\"Label\"] = 1\n",
    "    df_majority[\"source\"] = \"centroid\"\n",
    "\n",
    "    # dopasowanie kolumn\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "    out = pd.concat([df_majority, original_df], axis=0, ignore_index=True)\n",
    "    df_[name] = out\n",
    "\n",
    "    # --- Zapis do pliku ---\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "    if file_path.exists():\n",
    "        print(f\"File already exists: {file_path.name}\")\n",
    "    else:\n",
    "        out.to_csv(file_path, index=False)\n",
    "        print(f\"Saved: {file_path.name}\")\n",
    "\n",
    "print(\"\\n✅ Zakończono wszystkie zbiory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threadpoolctl import threadpool_info\n",
    "print(threadpool_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543cd1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1%: 252.7s  → est. full: 330.7 min (dla tych samych wątków)\n"
     ]
    }
   ],
   "source": [
    "import time, numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# X_full: Twoja macierz (np. float32, C-contiguous)\n",
    "X = np.ascontiguousarray(X_majority.to_numpy(np.float32, copy=False))\n",
    "n = X.shape[0]\n",
    "n_s = max(50_000, n//100)   # ~1%\n",
    "\n",
    "idx = np.random.RandomState(0).choice(n, n_s, replace=False)\n",
    "X_s = X[idx]\n",
    "\n",
    "k = int(13096)\n",
    "with threadpool_limits(limits=16):             # dostosuj do swojego ustawienia\n",
    "    t0 = time.perf_counter()\n",
    "    KMeans(n_clusters=k, algorithm=\"elkan\", n_init=\"auto\", max_iter=100, random_state=42).fit(X_s)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "t_est = (t1 - t0) * (n / n_s)                  # skalowanie po n\n",
    "print(f\"1%: {t1-t0:.1f}s  → est. full: {t_est/60:.1f} min (dla tych samych wątków)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5dfa71",
   "metadata": {},
   "source": [
    "### KMeans + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4542b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original minority class\n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(cluster_data_[name][i])>1):     # If more than one sample in the cluster\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         # Colculate Euclidean distance\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]    # Select the nearest neighbor to the centroid\n",
    "            \n",
    "        else:           # If only one sample in the cluster\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_NN_[name].values(), ignore_index=True)    \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"Label\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_nn.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa966ef",
   "metadata": {},
   "source": [
    "### KMeans + Cosinus Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83195f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original majority class\n",
    "    for i in range(len(centroids_)):                \n",
    "        if (len(cluster_data_[name][i])>1):       # If more than one sample in the cluster\n",
    "            dist_ = {}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         # Colculate Cosine similarity\n",
    "                \n",
    "            min_key = max(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]   # Select the nearest neighbor to the centroid\n",
    "            \n",
    "        else:       # If only one sample in the cluster\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)       \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ed5a8",
   "metadata": {},
   "source": [
    "### KMeans + cos + Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86592b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&cos&mal\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_MAN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5  #wazenie\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "        \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original majority class\n",
    "    for i in range(len(centroids_)):               \n",
    "        if (len(cluster_data_[name][i])>1):       # If more than one sample in the cluster\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)[0,0]          # using Cosine similarity\n",
    "                dist_[j] = distance.mahalanobis(centroid.flatten(), row.flatten(), np.linalg.pinv(np.cov(X_train.T)))  # using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)   \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_cos_mal.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d667d1c",
   "metadata": {},
   "source": [
    "### HDBSCAN + Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/hdbscan&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "labels_={}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply HDBSCAN clustering\n",
    "    hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    \n",
    "    # Save HDBSCAN model\n",
    "    file_path2 = base2 / f\"{name}_hdbscan_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(hdbscan, f)\n",
    "    \n",
    "    # Get labels and number of clusters\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels_[name] = len(unique_lables[unique_lables >=0])\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    print(f\"{name}: {labels_[name]}\")\n",
    "    \n",
    "    # Centroids from HDBSCAN\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "\n",
    "    # Add rows to cluster dictionary\n",
    "    for i in range(labels):\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "\n",
    "        target = count2\n",
    "        \n",
    "        # Sorted neighbors dictionary\n",
    "        per_cluster_sorted = {}\n",
    "        \n",
    "        # Calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # Calculate Euclidean distances from centroid to all points in the cluster\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin chooice to target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # Which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:\n",
    "            break\n",
    "\n",
    "    # Choose selected records of majority class (in order of selection)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)\n",
    "    results_[name] = maj_selected   \n",
    "    \n",
    "    # Combine reduced majority class with original df    \n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    #print(df_[name])\n",
    "\n",
    "    print(df_[name].info())\n",
    "\n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_HDBSCAN_NN.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e177b",
   "metadata": {},
   "source": [
    "### HDBSCAN + Cosinus Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/hdbscan&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_HDBSCAN_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply HDBSCAN clustering\n",
    "    hdbscan = HDBSCAN(store_centers=\"centroid\")\n",
    "    hdbscan_res = hdbscan.fit(X_majority)\n",
    "    \n",
    "    # Save HDBSCAN model\n",
    "    file_path2 = base2 / f\"{name}_hdbscan_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(hdbscan, f)\n",
    "        \n",
    "    # Get labels and number of clusters\n",
    "    labels = hdbscan_res.labels_\n",
    "    unique_lables = np.unique(labels)\n",
    "    labels = len(unique_lables[unique_lables >=0])\n",
    "    centroids_ = pd.DataFrame(hdbscan_res.centroids_, columns=X_train.columns)\n",
    "    \n",
    "    centroids_hdbscan = hdbscan.fit_predict(X_majority)\n",
    "\n",
    "    # Centroids from HDBSCAN + Add rows to cluster dictionary\n",
    "    for i in range(labels):\n",
    "        # Add rows to cluster dictionary\n",
    "        rows_in_cluster = X_majority[hdbscan.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        target = count2\n",
    "\n",
    "        # Sorted crucial indexes from clusters\n",
    "        per_cluster_sorted = {}\n",
    "        \n",
    "        # Calculate the nieghbors for each centroid (centroid -> rows_in_cluster)\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "\n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # Calculate Cosine distances from centroid to all points in the cluster\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()  # indeksy X_majority w kolejności rosnących dystansów\n",
    "        per_cluster_sorted[i] = order\n",
    "\n",
    "    # Round-robin chooice to target_n\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                 # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed: \n",
    "            break\n",
    "\n",
    "    # Choose selected records of majority class (in order of selection)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)\n",
    "    results_[name] = maj_selected   \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_HDBSCAN_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d665fef",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e260bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&hdbscan&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "        \n",
    "    # Centroids from KMeans + Add rows to cluster dictionary\n",
    "    for i in range (labels_[name]):\n",
    "        # Add rows to cluster dictionary\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    target = count2\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy()\n",
    "        # Calculate Euclidean distances from centroid to all points in the cluster\n",
    "        dists = rows.apply(lambda r: euclidean(centroid, r.to_numpy()), axis=1)\n",
    "        order = dists.sort_values().index.tolist()  # indexes of X_majority in ascending order of distances\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed: \n",
    "            break\n",
    "    \n",
    "    # Choose selected records of majority class (in order of selection)   \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   \n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "        \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info()) \n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_kmeans&hdbscan_nn.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee984bb5",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&hdbscan&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_DIST_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples) \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids from KMeans + Add rows to cluster dictionary\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    target = count2\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):\n",
    "        rows = cluster_data_[name][i]\n",
    "        if len(rows) == 0:\n",
    "            per_cluster_sorted[i] = []\n",
    "            continue\n",
    "        if len(rows) == 1:\n",
    "            per_cluster_sorted[i] = [rows.index[0]]\n",
    "            continue\n",
    "             \n",
    "        centroid = centroids_.iloc[i].to_numpy().reshape(1, -1)\n",
    "        # Calculate Cosine distances from centroid to all points in the cluster\n",
    "        dists = pairwise_distances(rows.values, centroid, metric=\"cosine\").ravel()\n",
    "        order = rows.index[np.argsort(dists)].tolist()              # indexes of X_majority in ascending order of distances\n",
    "        per_cluster_sorted[i] = order\n",
    "        \n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted[i]\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  # wszystkie klastry wyczerpane\n",
    "            break\n",
    "        \n",
    "    # Select selected records of majority class (in order of selection)    \n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)   # <-- CHANGED\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    print(df_[name].info()) \n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_kmeans&hdbscan_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe78987",
   "metadata": {},
   "source": [
    "### KMeans (samples=HDBSCAN()) + cosinus + Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a54093",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/unsw/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&hdbscan&cos&mal\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KMEANS_HDBSCAN_COS_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=labels_[name])\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids from KMeans + Add rows to cluster dictionary\n",
    "    for i in range (labels_[name]):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "        \n",
    "    target = count2\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    per_cluster_sorted = {}\n",
    "    for i in range(len(centroids_)):                # for each centroid\n",
    "        if (len(cluster_data_[name][i])>1):        # check if more than one sample in the cluster\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {}                # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)[0,0]         #using Cosine similarity\n",
    "                dist_[j] = distance.mahalanobis(centroid.flatten(), row.flatten(), np.linalg.pinv(np.cov(X_train.T)))  #using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "            order = sorted(comb_score_, key=comb_score_.get, reverse=True)\n",
    "            per_cluster_sorted[i] = [cluster_data_[name][i].index[j] for j in order]\n",
    "                \n",
    "        else:\n",
    "            results_KMEANS_HDBSCAN_COS_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "    results_[name] = pd.concat(results_KMEANS_HDBSCAN_COS_NN_[name].values(), ignore_index=True)\n",
    "    \n",
    "    # Sorted crucial indexes from clusters\n",
    "    selected_idx = []\n",
    "    ptr = {i: 0 for i in range(len(centroids_))}                     # which \"index\" to take now from the cluster\n",
    "    while len(selected_idx) < target:\n",
    "        progressed = False\n",
    "        for i in range(len(centroids_)):\n",
    "            lst = per_cluster_sorted.get(i, [])\n",
    "            j = ptr[i]\n",
    "            if j < len(lst):\n",
    "                idx = lst[j]\n",
    "                if idx not in selected_idx:\n",
    "                    selected_idx.append(idx)\n",
    "                ptr[i] += 1\n",
    "                progressed = True\n",
    "                if len(selected_idx) >= target:\n",
    "                    break\n",
    "        if not progressed:  \n",
    "            break\n",
    "\n",
    "    # Choose selected records of majority class (in order of selection)\n",
    "    maj_selected = X_majority.loc[selected_idx].reset_index(drop=True)\n",
    "    results_[name] = maj_selected     \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1.0] * len(results_[name]), name=\"target\")\n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    print(df_[name].info()) \n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_kmeans&hdbscan_cos&mal.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
