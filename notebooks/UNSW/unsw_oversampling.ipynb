{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b6e87b",
   "metadata": {},
   "source": [
    "# Create synthetic samples from UNSW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ca50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import itertools\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances, make_scorer, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, f_oneway, friedmanchisquare, wilcoxon\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44dc00d",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc78ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mati\\AppData\\Local\\Temp\\ipykernel_19304\\3554131459.py:2: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_unsw_1 = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_1.csv')\n",
      "C:\\Users\\Mati\\AppData\\Local\\Temp\\ipykernel_19304\\3554131459.py:3: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_unsw_2 = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_2.csv')\n"
     ]
    }
   ],
   "source": [
    "#df_unsw_features = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\NUSW-NB15_features.csv')\n",
    "df_unsw_1 = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_1.csv')\n",
    "df_unsw_2 = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_2.csv')\n",
    "df_unsw_3 = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_3.csv')\n",
    "df_unsw_4 = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fae3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "srcip",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sport",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dstip",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dsport",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "proto",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dur",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sbytes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dbytes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sttl",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dttl",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sloss",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dloss",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "service",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sload",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dload",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Spkts",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dpkts",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "swin",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dwin",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "stcpb",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dtcpb",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "smeansz",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dmeansz",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trans_depth",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "res_bdy_len",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sjit",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Djit",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Stime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ltime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sintpkt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dintpkt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tcprtt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "synack",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ackdat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_sm_ips_ports",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_state_ttl",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_flw_http_mthd",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is_ftp_login",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ct_ftp_cmd",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ct_srv_src",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_srv_dst",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_dst_ltm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_src_ ltm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_src_dport_ltm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_dst_sport_ltm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ct_dst_src_ltm",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "fbec37e8-15de-48bd-a6d7-45199688d7a0",
       "rows": [
        [
         "0",
         "59.166.0.0",
         "1390",
         "149.171.126.6",
         "53",
         "udp",
         "CON",
         "0.001055",
         "132",
         "164",
         "31",
         "29",
         "0",
         "0",
         "dns",
         "500473.9375",
         "621800.9375",
         "2",
         "2",
         "0",
         "0",
         "0",
         "0",
         "66",
         "82",
         "0",
         "0",
         "0.0",
         "0.0",
         "1421927414",
         "1421927414",
         "0.017",
         "0.013",
         "0.0",
         "0.0",
         "0.0",
         "0",
         "0",
         "0.0",
         "0.0",
         "0",
         "3",
         "7",
         "1",
         "3",
         "1",
         "1",
         "1",
         "0"
        ],
        [
         "1",
         "59.166.0.0",
         "33661",
         "149.171.126.9",
         "1024",
         "udp",
         "CON",
         "0.036133",
         "528",
         "304",
         "31",
         "29",
         "0",
         "0",
         "-",
         "87676.08594",
         "50480.17188",
         "4",
         "4",
         "0",
         "0",
         "0",
         "0",
         "132",
         "76",
         "0",
         "0",
         "9.89101",
         "10.682733",
         "1421927414",
         "1421927414",
         "7.005",
         "7.564333",
         "0.0",
         "0.0",
         "0.0",
         "0",
         "0",
         "0.0",
         "0.0",
         "0",
         "2",
         "4",
         "2",
         "3",
         "1",
         "1",
         "2",
         "0"
        ],
        [
         "2",
         "59.166.0.6",
         "1464",
         "149.171.126.7",
         "53",
         "udp",
         "CON",
         "0.001119",
         "146",
         "178",
         "31",
         "29",
         "0",
         "0",
         "dns",
         "521894.5313",
         "636282.375",
         "2",
         "2",
         "0",
         "0",
         "0",
         "0",
         "73",
         "89",
         "0",
         "0",
         "0.0",
         "0.0",
         "1421927414",
         "1421927414",
         "0.017",
         "0.013",
         "0.0",
         "0.0",
         "0.0",
         "0",
         "0",
         "0.0",
         "0.0",
         "0",
         "12",
         "8",
         "1",
         "2",
         "2",
         "1",
         "1",
         "0"
        ],
        [
         "3",
         "59.166.0.5",
         "3593",
         "149.171.126.5",
         "53",
         "udp",
         "CON",
         "0.001209",
         "132",
         "164",
         "31",
         "29",
         "0",
         "0",
         "dns",
         "436724.5625",
         "542597.1875",
         "2",
         "2",
         "0",
         "0",
         "0",
         "0",
         "66",
         "82",
         "0",
         "0",
         "0.0",
         "0.0",
         "1421927414",
         "1421927414",
         "0.043",
         "0.014",
         "0.0",
         "0.0",
         "0.0",
         "0",
         "0",
         "0.0",
         "0.0",
         "0",
         "6",
         "9",
         "1",
         "1",
         "1",
         "1",
         "1",
         "0"
        ],
        [
         "4",
         "59.166.0.3",
         "49664",
         "149.171.126.0",
         "53",
         "udp",
         "CON",
         "0.001169",
         "146",
         "178",
         "31",
         "29",
         "0",
         "0",
         "dns",
         "499572.25",
         "609067.5625",
         "2",
         "2",
         "0",
         "0",
         "0",
         "0",
         "73",
         "89",
         "0",
         "0",
         "0.0",
         "0.0",
         "1421927414",
         "1421927414",
         "0.005",
         "0.003",
         "0.0",
         "0.0",
         "0.0",
         "0",
         "0",
         "0.0",
         "0.0",
         "0",
         "7",
         "9",
         "1",
         "1",
         "1",
         "1",
         "1",
         "0"
        ]
       ],
       "shape": {
        "columns": 48,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>...</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.166.0.0</td>\n",
       "      <td>1390</td>\n",
       "      <td>149.171.126.6</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.0</td>\n",
       "      <td>33661</td>\n",
       "      <td>149.171.126.9</td>\n",
       "      <td>1024</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>528</td>\n",
       "      <td>304</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.166.0.6</td>\n",
       "      <td>1464</td>\n",
       "      <td>149.171.126.7</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.5</td>\n",
       "      <td>3593</td>\n",
       "      <td>149.171.126.5</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.166.0.3</td>\n",
       "      <td>49664</td>\n",
       "      <td>149.171.126.0</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        srcip  sport          dstip dsport proto state       dur  sbytes  \\\n",
       "0  59.166.0.0   1390  149.171.126.6     53   udp   CON  0.001055     132   \n",
       "1  59.166.0.0  33661  149.171.126.9   1024   udp   CON  0.036133     528   \n",
       "2  59.166.0.6   1464  149.171.126.7     53   udp   CON  0.001119     146   \n",
       "3  59.166.0.5   3593  149.171.126.5     53   udp   CON  0.001209     132   \n",
       "4  59.166.0.3  49664  149.171.126.0     53   udp   CON  0.001169     146   \n",
       "\n",
       "   dbytes  sttl  ...  is_ftp_login  ct_ftp_cmd  ct_srv_src ct_srv_dst  \\\n",
       "0     164    31  ...           0.0           0           3          7   \n",
       "1     304    31  ...           0.0           0           2          4   \n",
       "2     178    31  ...           0.0           0          12          8   \n",
       "3     164    31  ...           0.0           0           6          9   \n",
       "4     178    31  ...           0.0           0           7          9   \n",
       "\n",
       "   ct_dst_ltm  ct_src_ ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
       "0           1            3                 1                 1   \n",
       "1           2            3                 1                 1   \n",
       "2           1            2                 2                 1   \n",
       "3           1            1                 1                 1   \n",
       "4           1            1                 1                 1   \n",
       "\n",
       "   ct_dst_src_ltm  Label  \n",
       "0               1      0  \n",
       "1               2      0  \n",
       "2               1      0  \n",
       "3               1      0  \n",
       "4               1      0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unsw = pd.concat([df_unsw_1, df_unsw_2, df_unsw_3, df_unsw_4], ignore_index=True)\n",
    "df_unsw = df_unsw.drop(columns=['attack_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b1234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n",
      "C:\\Users\\mateu\\AppData\\Local\\Temp\\ipykernel_20668\\2688879982.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_other_ports[col] = 0\n"
     ]
    }
   ],
   "source": [
    "# what to do with ports? src drop because is random, dst - keep because is related to service, port number >1000 is random\n",
    "def to_int_or_hex(x):\n",
    "    try:\n",
    "        if isinstance(x, (int, float)) and not pd.isna(x):\n",
    "            return int(x)\n",
    "        elif isinstance(x, str) and x.startswith('0x'):\n",
    "            return int(x, 16)\n",
    "        elif isinstance(x, str):\n",
    "            return int(x)\n",
    "        else:\n",
    "            return pd.NA\n",
    "    except ValueError:\n",
    "        return pd.NA\n",
    "\n",
    "df_unsw['dsport'] = df_unsw['dsport'].apply(to_int_or_hex).astype('Int64')\n",
    "\n",
    "mask_well_known = df_unsw['dsport'] <= 1000\n",
    "df_well_known = df_unsw[mask_well_known].copy()\n",
    "df_other_ports = df_unsw[~mask_well_known].copy()\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_ports = encoder.fit_transform(df_well_known[['dsport']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_ports,\n",
    "    columns=encoder.get_feature_names_out(['dsport']),\n",
    "    index=df_well_known.index\n",
    ")\n",
    "\n",
    "df_well_known = pd.concat([encoded_df, df_well_known.drop(columns=['dsport'])], axis=1)\n",
    "\n",
    "df_other_ports['dsport_other'] = 1\n",
    "\n",
    "for col in df_well_known.columns:\n",
    "    if col not in df_other_ports.columns:\n",
    "        df_other_ports[col] = 0\n",
    "\n",
    "df_unsw = pd.concat([df_well_known, df_other_ports], axis=0).sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382de5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert 'dsport_other' next to 'proto' and fill NaN with 0\n",
    "col = df_unsw.pop('dsport_other')\n",
    "dst_col = df_unsw.columns.get_loc('proto')\n",
    "df_unsw.insert(dst_col, 'dsport_other', col)\n",
    "df_unsw['dsport_other'] = df_unsw['dsport_other'].fillna(0)\n",
    "df_unsw = df_unsw.drop(columns=['dsport', 'srcip', 'dstip', 'sport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2895eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" proto_index = df_unsw.columns.get_loc('proto')\\n\\nfor i, col in enumerate(df_proto.columns):\\n    df_unsw.insert(proto_index + 1 + i, col, df_unsw[col]) \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_unsw['proto_simplified'] = df_unsw['proto'].apply(\n",
    "    lambda x: x if x in ['tcp', 'udp'] else 'other'\n",
    ")\n",
    "\n",
    "df_unsw = pd.get_dummies(df_unsw, columns=['proto_simplified'], prefix='proto')\n",
    "#df_unsw = df_unsw.loc[:, ~df_unsw.columns.duplicated(keep='first')]\n",
    "\n",
    "col = df_unsw.pop('proto_other')\n",
    "col2 = df_unsw.pop('proto_tcp')\n",
    "col3 = df_unsw.pop('proto_udp')\n",
    "dst_col = df_unsw.columns.get_loc('state')\n",
    "df_unsw.insert(dst_col, 'proto_other', col)\n",
    "df_unsw.insert(dst_col + 1, 'proto_tcp', col2)\n",
    "df_unsw.insert(dst_col + 2, 'proto_udp', col3)\n",
    "\n",
    "df_unsw = df_unsw.drop(columns=['proto'])\n",
    "\n",
    "df_unsw.loc[:, df_unsw.columns.str.startswith('proto')] = (\n",
    "    df_unsw.loc[:, df_unsw.columns.str.startswith('proto')].astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ports = encoder.fit_transform(df_unsw[['proto']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_ports,\n",
    "    columns=encoder.get_feature_names_out(['proto']),\n",
    "    index=df_unsw.index\n",
    ")\n",
    "proto_index = df_unsw.columns.get_loc('proto')\n",
    "\n",
    "for i, col in enumerate(encoded_df.columns):\n",
    "    df_unsw.insert(proto_index + 1 + i, col, encoded_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42effe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced protocols to tcp, udp, other\n",
    "df_unsw['proto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96828d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsw = pd.get_dummies(df_unsw, columns=['proto'], prefix='proto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b0883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [\n",
    "    'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss',\n",
    "    'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb',\n",
    "    'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit',\n",
    "    'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
    "    'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', \n",
    "    'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ ltm',\n",
    "    'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_unsw[cols_to_normalize] = scaler.fit_transform(df_unsw[cols_to_normalize])\n",
    "\n",
    "with open(f\"D:\\\\ml\\\\undersampling_data\\\\models\\\\unsw\\\\scaler.pkl\", \"wb\") as f:\n",
    "            pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc72289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsw.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "df_unsw['ct_ftp_cmd'] = df_unsw['ct_ftp_cmd'].fillna(0)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "df_unsw[['ct_ftp_cmd']] = scaler2.fit_transform(df_unsw[['ct_ftp_cmd']])\n",
    "\n",
    "with open(f\"D:\\\\ml\\\\undersampling_data\\\\models\\\\unsw\\\\scaler_ct_ftp_cmd.pkl\", \"wb\") as f:\n",
    "            pickle.dump(scaler2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92f69d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states = ['FIN', 'CON', 'INT', 'REQ', 'RST', 'CLO', 'ACC']\n",
    "df_unsw = df_unsw[df_unsw['state'].isin(valid_states)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bbef502",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_state = encoder.fit_transform(df_unsw[['state']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_state,\n",
    "    columns=encoder.get_feature_names_out(['state']),\n",
    "    index=df_unsw.index\n",
    ")\n",
    "state_index = df_unsw.columns.get_loc('state')\n",
    "\n",
    "for i, col in enumerate(encoded_df.columns):\n",
    "    df_unsw.insert(state_index + 1 + i, col, encoded_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6918834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsw = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_processed_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ab6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsw['dsport_20.0'] = (df_unsw['service'] == 'ftp-data').astype(int)\n",
    "\n",
    "col = df_unsw.pop('dsport_20.0')\n",
    "dst_col = df_unsw.columns.get_loc('dsport_21.0')\n",
    "df_unsw.insert(dst_col, 'dsport_20.0', col)\n",
    "\n",
    "df_unsw = df_unsw[~df_unsw['service'].isin(['radius', 'irc'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unsw['ct_flw_http_mthd'] = df_unsw['ct_flw_http_mthd'].fillna(0)\n",
    "df_unsw['is_ftp_login'] = df_unsw['is_ftp_login'].fillna(0)\n",
    "df_unsw = df_unsw.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_unsw = df_unsw.drop(columns=['service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0c7517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1432298, 161), (613842, 161))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_unsw.drop(columns=['Label'])\n",
    "y = df_unsw['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,        # 20% danych do testu\n",
    "    random_state=42,      # dla powtarzalności\n",
    "    stratify=y            # zachowaj proporcje klas\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['service'])\n",
    "X_test = X_test.drop(columns=['service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e26d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\X_train.csv', index=False)\n",
    "X_test.to_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\X_test.csv', index=False)\n",
    "y_train.to_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\y_train.csv', index=False)\n",
    "y_test.to_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\y_test.csv', index=False)\n",
    "df_unsw.to_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dabe8",
   "metadata": {},
   "source": [
    "### Oversamling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bb0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\X_train.csv')\n",
    "y_train = pd.read_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e6d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92cf10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\UNSW-NB15_train_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db27e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Counter({0: 1370400, 1: 61898})\n",
      "After Counter({0: 1370400, 1: 1370400})\n",
      "After generation 3x SMOTE Counter({1: 2678902, 0: 1370400})\n"
     ]
    }
   ],
   "source": [
    "file_path1 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\smote_data.csv\"\n",
    "file_path2 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\smote3_data.csv\"\n",
    "#zamiana jesli istnieje to wczytaj plik jesli nie to stworz\n",
    "coun = Counter(y_train)\n",
    "majority_class = max(coun, key=coun.get)\n",
    "minority_class = min(coun, key=coun.get)\n",
    "missing_samples = coun[majority_class] - coun[minority_class]\n",
    "print(\"Before\", coun)\n",
    "smote = SMOTE()\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "smote2 = SMOTE(sampling_strategy={minority_class: coun[minority_class] + 2 * missing_samples}, random_state=42)\n",
    "X_train_sm3, y_train_sm3 = smote2.fit_resample(X_train, y_train)\n",
    "\n",
    "train_data_smote = pd.concat([X_train_sm, y_train_sm], axis=1)          #polaczenie danych wygenerowanych X_train oraz y_train\n",
    "train_data_smote3 = pd.concat([X_train_sm3, y_train_sm3], axis=1)\n",
    "\n",
    "#smote generated data\n",
    "train_data_smote['generated_by_smote'] = ['original' if i < len(df) else 'smote' for i in range(len(train_data_smote))]\n",
    "smote_data = train_data_smote[train_data_smote['generated_by_smote'] == 'smote'].drop('generated_by_smote', axis=1)\n",
    "smote_data[\"source\"] = \"smote\"\n",
    "if not os.path.exists(file_path1):\n",
    "    smote_data.to_csv(file_path1, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path1}')\n",
    "\n",
    "#smote3 generated data\n",
    "train_data_smote3['generated_by_smote'] = ['original' if i < len(df) else 'smote' for i in range(len(train_data_smote3))]\n",
    "smote_data3 = train_data_smote3[train_data_smote3['generated_by_smote'] == 'smote'].drop('generated_by_smote', axis=1)\n",
    "smote_data3[\"source\"] = \"smote\"\n",
    "if not os.path.exists(file_path2):\n",
    "    smote_data3.to_csv(file_path2, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path2}')\n",
    "\n",
    "con1 = Counter(y_train_sm)\n",
    "print(\"After\", con1)\n",
    "con2 = Counter(y_train_sm3)\n",
    "print(\"After generation 3x SMOTE\", con2)\n",
    "#pd.Series(y_train_sm).value_counts().plot.bar()\n",
    "#pd.Series(y_train_sm3).value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b376d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0        1370400\n",
      "1          61898\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "coun3 = Counter(y_train)\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704c508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Counter({0: 1370400, 1: 61898})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Nie można odnaleźć określonego pliku\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Counter({'Label': 1})\n",
      "After generated 3x BorderlineSMOTE Counter({'Label': 1})\n",
      "Plik istnieje pod ścieżką: D:\\ml\\undersampling_data\\data\\unsw\\oversampling\\borderline_data.csv\n",
      "Plik istnieje pod ścieżką: D:\\ml\\undersampling_data\\data\\unsw\\oversampling\\borderline3_data.csv\n"
     ]
    }
   ],
   "source": [
    "file_path2 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\borderline_data.csv\"\n",
    "file_path1 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\borderline3_data.csv\"\n",
    "#generate new data by borderLineSMOTE\n",
    "coun3 = Counter(y_train['Label'])\n",
    "majority_class = max(coun3, key=coun3.get)\n",
    "minority_class = min(coun3, key=coun3.get)\n",
    "missing_samples = coun3[majority_class] - coun3[minority_class]\n",
    "print(\"Before\", coun3)\n",
    "\n",
    "brdsmote = BorderlineSMOTE(random_state=42)\n",
    "X_train_bsm, y_train_bsm = brdsmote.fit_resample(X_train, y_train)\n",
    "\n",
    "brdsmote3 = BorderlineSMOTE(sampling_strategy={minority_class: coun3[minority_class] + 2 * missing_samples}, random_state=42)\n",
    "X_train_bsm3, y_train_bsm3 = brdsmote3.fit_resample(X_train, y_train)\n",
    "\n",
    "con4 = Counter(y_train_bsm)\n",
    "print(\"After\", con4)\n",
    "con5 = Counter(y_train_bsm3)\n",
    "print(\"After generated 3x BorderlineSMOTE\", con5)\n",
    "\n",
    "train_data_borderline_smote = pd.concat([X_train_bsm, y_train_bsm], axis=1)          #polaczenie danych wygenerowanych X_train oraz y_train\n",
    "train_data_borderline_smote3 = pd.concat([X_train_bsm3, y_train_bsm3], axis=1)\n",
    "\n",
    "#borderline smote generated data\n",
    "train_data_borderline_smote['generated_by_borderline_smote'] = ['original' if i < len(df) else 'brd smote' for i in range(len(train_data_borderline_smote))]\n",
    "boarderline_smote_data = train_data_borderline_smote[train_data_borderline_smote['generated_by_borderline_smote'] == 'brd smote'].drop('generated_by_borderline_smote', axis=1)\n",
    "boarderline_smote_data[\"source\"]=\"borderline smote\"\n",
    "boarderline_smote_data = boarderline_smote_data[boarderline_smote_data['source'] != 'original']\n",
    "if not os.path.exists(file_path2):\n",
    "    boarderline_smote_data.to_csv(file_path2, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path2}')\n",
    "    \n",
    "#borderline smote3 generated data\n",
    "train_data_borderline_smote3['generated_by_borderline_smote'] = ['original' if i < len(df) else 'brd smote' for i in range(len(train_data_borderline_smote3))]\n",
    "boarderline_smote_data3 = train_data_borderline_smote3[train_data_borderline_smote3['generated_by_borderline_smote'] == 'brd smote'].drop('generated_by_borderline_smote', axis=1)\n",
    "boarderline_smote_data3[\"source\"]=\"borderline smote\"\n",
    "boarderline_smote_data3 = boarderline_smote_data3[boarderline_smote_data3['source'] != 'original']\n",
    "if not os.path.exists(file_path1):\n",
    "    boarderline_smote_data3.to_csv(file_path1, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebbd1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7181038446724415 GB\n"
     ]
    }
   ],
   "source": [
    "print(df.memory_usage(deep=True).sum() / (1024**3), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbdb85a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.3 GiB for an array with shape (61898, 37519) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ctgan\\data_transformer.py\", line 138, in _transform_discrete\n    return ohe.transform(data).to_numpy()\n           ~~~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\rdt\\transformers\\base.py\", line 57, in wrapper\n    return function(self, *args, **kwargs)\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\rdt\\transformers\\base.py\", line 424, in transform\n    transformed_data = self._transform(columns_data)\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\rdt\\transformers\\categorical.py\", line 682, in _transform\n    return self._transform_helper(data)\n           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"c:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\rdt\\transformers\\categorical.py\", line 650, in _transform_helper\n    array = (coded == dummies).astype(int)\nnumpy._core._exceptions._ArrayMemoryError: Unable to allocate 17.3 GiB for an array with shape (61898, 37519) and data type int64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_num[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m target_num[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     25\u001b[0m     data_y1 \u001b[38;5;241m=\u001b[39m df_gan[df_gan[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m     ctgan\u001b[38;5;241m.\u001b[39mfit(data_y1, columns_list)\n\u001b[0;32m     27\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(target_num[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mtarget_num[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     28\u001b[0m     df_GAN \u001b[38;5;241m=\u001b[39m ctgan\u001b[38;5;241m.\u001b[39msample(sample)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ctgan\\synthesizers\\base.py:50\u001b[0m, in \u001b[0;36mrandom_state.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m set_random_states(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_random_state):\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ctgan\\synthesizers\\ctgan.py:348\u001b[0m, in \u001b[0;36mCTGAN.fit\u001b[1;34m(self, train_data, discrete_columns, epochs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer \u001b[38;5;241m=\u001b[39m DataTransformer()\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39mfit(train_data, discrete_columns)\n\u001b[1;32m--> 348\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39mtransform(train_data)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_sampler \u001b[38;5;241m=\u001b[39m DataSampler(\n\u001b[0;32m    351\u001b[0m     train_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39moutput_info_list, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_frequency\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    354\u001b[0m data_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer\u001b[38;5;241m.\u001b[39moutput_dimensions\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ctgan\\data_transformer.py:187\u001b[0m, in \u001b[0;36mDataTransformer.transform\u001b[1;34m(self, raw_data)\u001b[0m\n\u001b[0;32m    183\u001b[0m     column_data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronous_transform(\n\u001b[0;32m    184\u001b[0m         raw_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_transform_info_list\n\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     column_data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_transform(raw_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_transform_info_list)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(column_data_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ctgan\\data_transformer.py:172\u001b[0m, in \u001b[0;36mDataTransformer._parallel_transform\u001b[1;34m(self, raw_data, column_transform_info_list)\u001b[0m\n\u001b[0;32m    169\u001b[0m         process \u001b[38;5;241m=\u001b[39m delayed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_discrete)(column_transform_info, data)\n\u001b[0;32m    170\u001b[0m     processes\u001b[38;5;241m.\u001b[39mappend(process)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(processes)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mati\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.3 GiB for an array with shape (61898, 37519) and data type int64"
     ]
    }
   ],
   "source": [
    "file_path3 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\GAN_data.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "file_path4 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\GAN3_data.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "#generate new data by GAN\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#data preparation\n",
    "#df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df_gan = df\n",
    "\n",
    "#GAN\n",
    "columns_list = df_gan.columns\n",
    "target_num = df_gan['Label'].value_counts()\n",
    "ctgan = CTGAN(\n",
    "    epochs=10,\n",
    "    batch_size=16,                 # 32–64 jeśli nadal ciasno\n",
    "    generator_dim=(64, 64),      # mniejsze warstwy niż domyślne\n",
    "    discriminator_dim=(64, 64),\n",
    "    embedding_dim=16,              # 32–64 zmniejsza wektory warunkujące\n",
    "    pac=1,                         # >1 poprawia stabilność, ale ZWIĘKSZA pamięć\n",
    "    cuda=True                      # ustaw False jeśli GPU się dławi; to nie rozwiąże RAM, ale VRAM tak\n",
    ")\n",
    "\n",
    "if target_num[0] > target_num[1]:\n",
    "    data_y1 = df_gan[df_gan['Label']==1]\n",
    "    ctgan.fit(data_y1, columns_list)\n",
    "    sample = abs(target_num[0]-target_num[1])\n",
    "    df_GAN = ctgan.sample(sample)\n",
    "    print('Dane wygenerowane: ', df_GAN['Label'].value_counts())\n",
    "    balanced_data = pd.concat([df_gan, df_GAN], ignore_index=False)\n",
    "else:\n",
    "    data_y0 = df_gan[df_gan['Label']==0]\n",
    "    ctgan.fit(data_y0, columns_list)\n",
    "    sample = abs(target_num[0]-target_num[1])\n",
    "    df_GAN = ctgan.sample(sample)\n",
    "    print('Dane wygenerowane: ', df_GAN['Label'].value_counts())\n",
    "    balanced_data = pd.concat([df_gan, df_GAN], ignore_index=False)\n",
    "\n",
    "#GAN3\n",
    "if target_num[0] > target_num[1]:\n",
    "    data_y1 = df_gan[df_gan['Label']==1]\n",
    "    ctgan.fit(data_y1, columns_list)\n",
    "    sample = abs(target_num[0]-target_num[1])\n",
    "    df_GAN3 = ctgan.sample(sample*2)\n",
    "    print('Dane wygenerowane: ', df_GAN3['Label'].value_counts())\n",
    "    balanced_data3 = pd.concat([df_gan, df_GAN3], ignore_index=False)\n",
    "else:\n",
    "    data_y0 = df_gan[df_gan['Label']==0]\n",
    "    ctgan.fit(data_y0, columns_list)\n",
    "    sample = abs(target_num[0]-target_num[1])\n",
    "    df_GAN3 = ctgan.sample(sample*2)\n",
    "    print('Dane wygenerowane: ', df_GAN3['Label'].value_counts())\n",
    "    balanced_data3 = pd.concat([df_gan, df_GAN3], ignore_index=False)\n",
    "\n",
    "#balanced_data = balanced_data.drop(columns=[\"source\"])  \n",
    "y_train_gan = balanced_data[\"Label\"]\n",
    "X_train_gan = balanced_data.drop(columns=[\"Label\"])\n",
    "\n",
    "#GAN3\n",
    "y_train_gan3 = balanced_data3[\"Label\"]\n",
    "X_train_gan3 = balanced_data3.drop(columns=[\"Label\"])\n",
    "con5 = Counter(y_train)\n",
    "print(\"Before\", con5)\n",
    "con6 = Counter(y_train_gan)\n",
    "print(\"After\", con6)\n",
    "gan_data = df_GAN\n",
    "con7 = Counter(y_train_gan3)\n",
    "print(\"After\", con7)\n",
    "gan_data3 = df_GAN3\n",
    "\n",
    "gan_data[\"source\"] = \"gan\"\n",
    "if not os.path.exists(file_path3):\n",
    "    gan_data.to_csv(file_path3, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path3}')\n",
    "    \n",
    "gan_data3[\"source\"] = \"gan\"\n",
    "if not os.path.exists(file_path4):\n",
    "    gan_data3.to_csv(file_path4, index=False)\n",
    "else:\n",
    "    print(f'Plik istnieje pod ścieżką: {file_path4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f080983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({0: 1370400, 1: 61898})\n",
      "After (GAN): Counter({0: 1370400, 1: 1370400})\n",
      "After (GAN3): Counter({1: 2678902, 0: 1370400})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mati\\AppData\\Local\\Temp\\ipykernel_25284\\605176821.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_GAN[\"source\"]  = \"gan\"\n",
      "C:\\Users\\Mati\\AppData\\Local\\Temp\\ipykernel_25284\\605176821.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_GAN3[\"source\"] = \"gan\"\n"
     ]
    }
   ],
   "source": [
    "file_path3 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\GAN_data.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "file_path4 = \"D:\\\\ml\\\\undersampling_data\\\\data\\\\unsw\\\\oversampling\\\\GAN3_data.csv\"        #sciezka wraz z nazwa pod jaka wygenerowac plik\n",
    "\n",
    "# Chunking parameters\n",
    "chunk_rows = 30000      # ile wierszy mniejszości na jedną porcję (dostosuj do VRAM/RAM)\n",
    "max_shards = None       # np. 10 by ograniczyć liczbę porcji; None = bez limitu\n",
    "\n",
    "# CTGAN\n",
    "CTGAN_KW = dict(\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    generator_dim=(64, 64),\n",
    "    discriminator_dim=(64, 64),\n",
    "    embedding_dim=16,\n",
    "    pac=1,\n",
    "    cuda=True,          # będzie zbijane na False przy OOM\n",
    ")\n",
    "\n",
    "# ===== pomocnicze =====\n",
    "def detect_discrete_columns(df, label_col=\"Label\", max_int_card=50):\n",
    "    \"\"\"Zbuduj listę kolumn dyskretnych dla CTGAN (stare API oczekuje takiej listy).\"\"\"\n",
    "    disc = []\n",
    "    for c in df.columns:\n",
    "        if c == label_col:\n",
    "            disc.append(c); continue\n",
    "        dt = df[c].dtype\n",
    "        if str(dt) in (\"object\", \"bool\") or \"category\" in str(dt):\n",
    "            disc.append(c)\n",
    "        elif np.issubdtype(dt, np.integer) and df[c].nunique(dropna=True) <= max_int_card:\n",
    "            disc.append(c)\n",
    "    return sorted(list(set(disc)))\n",
    "\n",
    "def make_ctgan():\n",
    "    try:\n",
    "        from ctgan import CTGAN\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Brak pakietu ctgan. Zainstaluj: pip install ctgan\") from e\n",
    "    return CTGAN(**CTGAN_KW)\n",
    "\n",
    "def fit_with_fallback(ctgan, data, discrete_columns):\n",
    "    \"\"\"Spróbuj na GPU, w razie OOM spadnij na CPU i oczyść pamięć GPU.\"\"\"\n",
    "    try:\n",
    "        ctgan.fit(data, discrete_columns)\n",
    "        return ctgan\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e).lower()\n",
    "        if \"cuda\" in msg or \"out of memory\" in msg or \"cublas\" in msg:\n",
    "            # fallback na CPU\n",
    "            try:\n",
    "                import torch\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "            # przeinstancjuj CTGAN na CPU\n",
    "            cpu_kwargs = {**CTGAN_KW, \"cuda\": False}\n",
    "            from ctgan import CTGAN\n",
    "            ctgan_cpu = CTGAN(**cpu_kwargs)\n",
    "            ctgan_cpu.fit(data, discrete_columns)\n",
    "            return ctgan_cpu\n",
    "        raise\n",
    "\n",
    "def generate_from_shards(minority_df, deficit_total, discrete_columns,\n",
    "                         multiplier=1, chunk_rows=30000, max_shards=None):\n",
    "    \"\"\"\n",
    "    minority_df: tylko mniejszość (Label == minority_label)\n",
    "    deficit_total: ile łącznie przykładów musimy dorobić, by zbalansować.\n",
    "    multiplier: 1 dla 'GAN' (wyrównanie), 2 dla 'GAN3' (×2 mniejszość).\n",
    "    \"\"\"\n",
    "    target_total = int(deficit_total * multiplier)\n",
    "    if target_total <= 0:\n",
    "        return pd.DataFrame(columns=minority_df.columns)\n",
    "\n",
    "    # Pokrój minority na porcje\n",
    "    n = len(minority_df)\n",
    "    if chunk_rows <= 0:\n",
    "        chunk_rows = n  # jedna porcja\n",
    "    shard_indices = list(range(0, n, chunk_rows))\n",
    "    if max_shards is not None:\n",
    "        shard_indices = shard_indices[:max_shards]\n",
    "\n",
    "    samples_left = target_total\n",
    "    generated_parts = []\n",
    "\n",
    "    for i, start in enumerate(shard_indices, 1):\n",
    "        end = min(start + chunk_rows, n)\n",
    "        shard = minority_df.iloc[start:end].copy()\n",
    "\n",
    "        # Rozdziel ile generujemy z tej porcji (proporcjonalnie do rozmiaru porcji)\n",
    "        # + nigdy nie przekraczaj tego co zostało\n",
    "        share = (len(shard) / n)\n",
    "        to_make = int(round(target_total * share))\n",
    "        to_make = min(to_make, samples_left)\n",
    "        if to_make <= 0:\n",
    "            continue\n",
    "\n",
    "        # Każda porcja: nowy, mały model (czyści pamięć po zakończeniu)\n",
    "        ctgan = make_ctgan()\n",
    "        ctgan = fit_with_fallback(ctgan, shard, discrete_columns)\n",
    "\n",
    "        synth = ctgan.sample(to_make)\n",
    "        generated_parts.append(synth)\n",
    "\n",
    "        samples_left -= to_make\n",
    "\n",
    "        # porządkowanie pamięci\n",
    "        del ctgan; gc.collect()\n",
    "        try:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if samples_left <= 0:\n",
    "            break\n",
    "\n",
    "    if samples_left > 0:\n",
    "        # Jeżeli zaokrąglenia zostawiły nam ogon — dorób z ostatniej porcji\n",
    "        last = minority_df.iloc[shard_indices[-1]:min(shard_indices[-1]+chunk_rows, n)].copy()\n",
    "        ctgan = make_ctgan()\n",
    "        ctgan = fit_with_fallback(ctgan, last, discrete_columns)\n",
    "        synth_tail = ctgan.sample(samples_left)\n",
    "        generated_parts.append(synth_tail)\n",
    "        del ctgan; gc.collect()\n",
    "        try:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not generated_parts:\n",
    "        return pd.DataFrame(columns=minority_df.columns)\n",
    "\n",
    "    out = pd.concat(generated_parts, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "# ===== GŁÓWNY PRZEPŁYW =====\n",
    "\n",
    "# Zakładam, że masz df w pamięci:\n",
    "# df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "df_gan = df.copy()\n",
    "\n",
    "label_col = \"Label\"\n",
    "counts = df_gan[label_col].value_counts().sort_index()\n",
    "if set(counts.index) != {0, 1}:\n",
    "    raise ValueError(f\"Oczekiwałem klas 0/1 w kolumnie '{label_col}', mam: {counts.index.tolist()}\")\n",
    "\n",
    "majority_label = counts.idxmax()\n",
    "minority_label = 1 - majority_label\n",
    "deficit = counts[majority_label] - counts[minority_label]\n",
    "\n",
    "# Kolumny dyskretne do CTGAN\n",
    "discrete_columns = detect_discrete_columns(df_gan, label_col=label_col, max_int_card=50)\n",
    "\n",
    "# Dane mniejszości\n",
    "minority_df = df_gan[df_gan[label_col] == minority_label].reset_index(drop=True)\n",
    "\n",
    "# --- Wersja GAN (wyrównanie 1:1) ---\n",
    "df_GAN = generate_from_shards(\n",
    "    minority_df=minority_df,\n",
    "    deficit_total=deficit,\n",
    "    discrete_columns=discrete_columns,\n",
    "    multiplier=1,\n",
    "    chunk_rows=chunk_rows,\n",
    "    max_shards=max_shards\n",
    ")\n",
    "\n",
    "balanced_data = pd.concat([df_gan, df_GAN], ignore_index=True)\n",
    "\n",
    "# --- Wersja GAN3 (×2 mniejszość) ---\n",
    "df_GAN3 = generate_from_shards(\n",
    "    minority_df=minority_df,\n",
    "    deficit_total=deficit,\n",
    "    discrete_columns=discrete_columns,\n",
    "    multiplier=2,\n",
    "    chunk_rows=chunk_rows,\n",
    "    max_shards=max_shards\n",
    ")\n",
    "\n",
    "balanced_data3 = pd.concat([df_gan, df_GAN3], ignore_index=True)\n",
    "\n",
    "# Raporty\n",
    "y_train = df_gan[label_col]                    # Twoje \"Before\" z oryginału\n",
    "y_train_gan  = balanced_data[label_col]\n",
    "y_train_gan3 = balanced_data3[label_col]\n",
    "\n",
    "print(\"Before:\", Counter(y_train))\n",
    "print(\"After (GAN):\", Counter(y_train_gan))\n",
    "print(\"After (GAN3):\", Counter(y_train_gan3))\n",
    "\n",
    "# Oznacz generatywne\n",
    "df_GAN[\"source\"]  = \"gan\"\n",
    "df_GAN3[\"source\"] = \"gan\"\n",
    "\n",
    "# Zapis\n",
    "if not os.path.exists(file_path3):\n",
    "    df_GAN.to_csv(file_path3, index=False)\n",
    "else:\n",
    "    print(f\"Plik istnieje pod ścieżką: {file_path3}\")\n",
    "\n",
    "if not os.path.exists(file_path4):\n",
    "    df_GAN3.to_csv(file_path4, index=False)\n",
    "else:\n",
    "    print(f\"Plik istnieje pod ścieżką: {file_path4}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
