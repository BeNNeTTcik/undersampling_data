{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6301aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "#from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b861e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    151\n",
      "1     46\n",
      "Name: count, dtype: int64\n",
      "197\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\original_data.csv\")    \n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote_data.csv\")\n",
    "GAN_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN_data.csv\")\n",
    "borderline_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline_data.csv\")\n",
    "smote2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote3_data.csv\")\n",
    "GAN2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN3_data.csv\")    \n",
    "borderline2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline3_data.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\y_test.csv\")\n",
    "\n",
    "#Before undersampling\n",
    "print(original_data[\"target\"].value_counts())\n",
    "count1=original_data[\"target\"].value_counts().sum()\n",
    "print(count1)\n",
    "count2=abs((original_data['target']==0).sum() - (original_data['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973a167",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c318c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed data\n",
    "mix_data = pd.concat([GAN_data, smote_data, borderline_data, original_data], axis=0, ignore_index=True)         \n",
    "mix_data = mix_data.reset_index(drop=True)\n",
    "\n",
    "#data with one oversampling method and original data e.g.(smote+original)\n",
    "smote_data = pd.concat([smote_data, smote2_data, original_data], axis=0, ignore_index=True)\n",
    "smote_data = smote_data.reset_index(drop=True)\n",
    "borderline_data = pd.concat([borderline_data, borderline2_data, original_data], axis=0, ignore_index=True)\n",
    "borderline_data = borderline_data.reset_index(drop=True)\n",
    "GAN_data = pd.concat([GAN_data, GAN2_data, original_data], axis=0, ignore_index=True)\n",
    "GAN_data = GAN_data.reset_index(drop=True)\n",
    "\n",
    "sum_all_data = pd.concat([smote_data, GAN_data, borderline_data, original_data], axis=0, ignore_index=True)\n",
    "sum_all_data = sum_all_data.drop_duplicates()\n",
    "\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_data.drop(columns=[\"target\", \"source\"]), mix_data[\"target\"]\n",
    "X_smote, y_smote = smote_data.drop(columns=[\"target\", \"source\"]), smote_data[\"target\"]\n",
    "X_GAN, y_GAN = GAN_data.drop(columns=[\"target\", \"source\"]), GAN_data[\"target\"]\n",
    "X_borderline, y_borderline = borderline_data.drop(columns=[\"target\", \"source\"]), borderline_data[\"target\"]\n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_data\n",
    "compare[\"smote\"] = smote_data\n",
    "compare[\"GAN\"] = GAN_data\n",
    "compare[\"borderline\"] = borderline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9702ef0",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec82b04",
   "metadata": {},
   "source": [
    "#### K-means + centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "centroids_rows_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "centroids_ = {}\n",
    "\n",
    "results_KM_SWAP_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "     # klasteryzacja dotyczy tylko jednego ze zbiorow drugi jest przepisywany\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    #centroids\n",
    "    for i in range ((int)((count1+count2)/2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        centroids_rows_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    #results_KM_SWAP_ = {}\n",
    "    \n",
    "    \n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(centroids_rows_[name][i])>1):\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(centroids_rows_[name][i])):\n",
    "                index_ = list(centroids_rows_[name][i].index)\n",
    "                row = centroids_rows_[name][i].iloc[j]\n",
    "                #index_ = { \"index_rows\": j, \"index_centroid\": list(centroids_rows_[name][i].index) } #tworze slwonik wartosci\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                \n",
    "                #print(index_)\n",
    "                dist_[j] = euclidean(centroid, row)         #tworze slwonik wartosci\n",
    "                \n",
    "            #print(index_map)    \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            #print(min_key)\n",
    "            #new_cent = index_.get\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[min_key]]\n",
    "            \n",
    "            #print(results_KM_SWAP_[i])\n",
    "            \n",
    "            #results_KM_SWAP_[i] = dist_.iloc[min_index]\n",
    "            #print(dist_[0])\n",
    "        else:\n",
    "            results_KM_SWAP_[name][i] = centroids_rows_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_SWAP_[name].values(), ignore_index=True)    \n",
    "        #results_KM_SWAP_[name][i][\"target\"] = 1      \n",
    "        #results_KM_SWAP_[name][i][\"source\"] = None\n",
    "        #print(results_KM_SWAP_[name])    \n",
    "    \n",
    "    \n",
    "    \"\"\" for key in [\"mix\", \"smote\", \"GAN\", \"borderline\"]:\n",
    "        df_[name] = pd.DataFrame.from_dict(results_KM_SWAP_[name], orient=\"index\")        \"\"\"   \n",
    "\n",
    "    \n",
    "        \n",
    "    df_y_majority = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "        \n",
    "    #print(df_majority)        \n",
    "    df_X_minority = X_minority.reset_index(drop=True)\n",
    "    df_y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    df_miniority = pd.concat([df_X_minority, df_y_minority], axis=1).reset_index(drop=True)\n",
    "    #print(df_miniority)\n",
    "        #print(df_[name])\n",
    "    df_[name] = pd.concat([df_majority, df_miniority], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    #copy source from sum_all_data \n",
    "    columns_ = list(df_[name].columns.values)\n",
    "    df_[name] = df_[name].merge(sum_all_data, on=columns_, how=\"left\")\n",
    "    print(df_[name])  \n",
    "    \n",
    "    df_[name].to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_SWAP_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4f00d",
   "metadata": {},
   "source": [
    "#### K-means + the nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)((count1+count2)/2), name=\"target\") \n",
    "    \n",
    "    X_minority = X_minority.reset_index(drop=True)\n",
    "    y_minority = pd.Series([0] * len(X_minority), name=\"target\")\n",
    "    \n",
    "    X_final = pd.concat([X_majority_reduced, X_minority], axis=0).reset_index(drop=True) \n",
    "    y_final = pd.concat([y_majority_reduced, y_minority], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Concat resampled data\n",
    "    reduced_data = pd.concat([X_final, y_final], axis=1)\n",
    "    \n",
    "    reduced_data[\"source\"] = None  # Initialize the source column with None\n",
    "    \n",
    "    # gdy target = 1 wtedy source = \"centroid\", inaczej source = \"original\"\n",
    "    \n",
    "    # Compare data to copy source column\n",
    "    data_nosource = compare_df\n",
    "    reduced_data_nosource = reduced_data\n",
    "\n",
    "    # Iterate through the rows in reduced_data_nosource\n",
    "    for index, row in reduced_data_nosource.iterrows():\n",
    "        match = data_nosource.eq(row).all(axis=1)  # Check where rows are identical\n",
    "        if match.any():  # If a match is found\n",
    "            matched_index = match.idxmax()  # Get the first matching index\n",
    "            reduced_data.loc[index, \"source\"] = compare_df.loc[matched_index, \"source\"]\n",
    "            \n",
    "    # Check for any rows that still have None in the source column\n",
    "    missing_source = reduced_data[reduced_data[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        reduced_data.loc[reduced_data[\"source\"].isna(), \"source\"] = \"centroid\"       \n",
    "    \n",
    "    #reduced_data.to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Data reduced for {name} data\")\n",
    "    print(reduced_data[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fabb30",
   "metadata": {},
   "source": [
    "#### K-means + cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22453c6",
   "metadata": {},
   "source": [
    "#### K-means + cosinus + distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c0c4d",
   "metadata": {},
   "source": [
    "#### DBSCAN + distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf27ed5",
   "metadata": {},
   "source": [
    "#### DBSCAN + cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb4069",
   "metadata": {},
   "source": [
    "#### K-means (resampling = calculate by DBSCAN) + centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23ab46",
   "metadata": {},
   "source": [
    "#### K-means (resampling = calculate by DBSCAN) + the nearest neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c243f5b",
   "metadata": {},
   "source": [
    "#### K-means (resampling = calculate by DBSCAN) + cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c152cd",
   "metadata": {},
   "source": [
    "#### K-means (resampling = calculate by DBSCAN) + cosinus + distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
