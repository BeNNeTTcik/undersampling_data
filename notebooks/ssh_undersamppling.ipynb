{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #undersampling prepare data //mixed data (gan, brdsmote, smote)\\ncc_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\clustercentroids_data.csv\")\\nif_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\isolationforest_data.csv\")\\nnm_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\nearmiss_data.csv\")\\nmedian_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\median_data.csv\")\\nlof_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\localoutlierfactor_data.csv\") '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oversampling data\n",
    "original_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\original_data.csv\")    \n",
    "original_data = original_data.drop(columns=[\"Unnamed: 0\"])\n",
    "smote_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote_data.csv\")\n",
    "GAN_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN_data.csv\")\n",
    "borderline_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline_data.csv\")\n",
    "smote2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\smote3_data.csv\")\n",
    "GAN2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\GAN3_data.csv\")    \n",
    "borderline2_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\borderline3_data.csv\")\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test.csv\")\n",
    "y_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\y_test.csv\")\n",
    "\n",
    "\n",
    "\"\"\" #undersampling prepare data //mixed data (gan, brdsmote, smote)\n",
    "cc_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\clustercentroids_data.csv\")\n",
    "if_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\isolationforest_data.csv\")\n",
    "nm_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\nearmiss_data.csv\")\n",
    "median_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\median_data.csv\")\n",
    "lof_data = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\localoutlierfactor_data.csv\") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    151\n",
      "1     46\n",
      "Name: count, dtype: int64\n",
      "197\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    }
   ],
   "source": [
    "print(original_data[\"target\"].value_counts())\n",
    "count1=original_data[\"target\"].value_counts().sum()\n",
    "print(count1)\n",
    "count2=abs((original_data['target']==0).sum() - (original_data['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixed data\n",
    "mix_data = pd.concat([GAN_data, smote_data, borderline_data, original_data], axis=0, ignore_index=True)         \n",
    "mix_data = mix_data.reset_index(drop=True)\n",
    "\n",
    "#data with one oversampling method and original data e.g.(smote+original)\n",
    "smote_data = pd.concat([smote_data, smote2_data, original_data], axis=0, ignore_index=True)\n",
    "smote_data = smote_data.reset_index(drop=True)\n",
    "borderline_data = pd.concat([borderline_data, borderline2_data, original_data], axis=0, ignore_index=True)\n",
    "borderline_data = borderline_data.reset_index(drop=True)\n",
    "GAN_data = pd.concat([GAN_data, GAN2_data, original_data], axis=0, ignore_index=True)\n",
    "GAN_data = GAN_data.reset_index(drop=True)\n",
    "\n",
    "sum_all_data = pd.concat([smote_data, GAN_data, borderline_data, mix_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mix, y_mix = mix_data.drop(columns=[\"target\", \"source\"]), mix_data[\"target\"]\n",
    "X_smote, y_smote = smote_data.drop(columns=[\"target\", \"source\"]), smote_data[\"target\"]\n",
    "X_GAN, y_GAN = GAN_data.drop(columns=[\"target\", \"source\"]), GAN_data[\"target\"]\n",
    "X_borderline, y_borderline = borderline_data.drop(columns=[\"target\", \"source\"]), borderline_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_data\n",
    "compare[\"smote\"] = smote_data\n",
    "compare[\"GAN\"] = GAN_data\n",
    "compare[\"borderline\"] = borderline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NearMiss version1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for mix data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n",
      "Data reduced for smote data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n",
      "Data reduced for GAN data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n",
      "Data reduced for borderline data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "NM = NearMiss(version=1)\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "    \n",
    "    X_NM, y_NM = NM.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #concat resampled data\n",
    "    nearmiss_data = pd.concat([X_NM, y_NM], axis=1)\n",
    "    \n",
    "    NM_data_nosource = compare_df.drop(columns=[\"source\"])\n",
    "    nearmiss_data_nosource = nearmiss_data\n",
    "\n",
    "    for index, row in nearmiss_data_nosource.iterrows():\n",
    "        match = NM_data_nosource.eq(row).all(axis=1)  # Sprawdza, gdzie wiersze są identyczne\n",
    "        if match.any():  # Jeśli znaleziono dopasowanie\n",
    "            matched_index = match.idxmax()  # Pobiera pierwszy pasujący indeks\n",
    "            nearmiss_data.loc[index, \"source\"] = compare_df.loc[matched_index, \"source\"]\n",
    "            \n",
    "    nearmiss_data.to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_NM_data.csv\")\n",
    "    \n",
    "    print(f\"Data reduced for {name} data\")\n",
    "    print(nearmiss_data[\"target\"].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for mix data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for smote data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for GAN data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for borderline data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "KM = KMeans(n_clusters=(int)((count1+count2)/2))\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "    \n",
    "    X_majority = X_train[y_train == 1]\n",
    "    X_minority = X_train[y_train == 0]\n",
    "    \n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([0] * (int)((count1+count2)/2), name=\"target\")\n",
    "    \n",
    "    X_minority = X_minority.reset_index(drop=True)\n",
    "    y_minority = pd.Series([1] * len(X_minority), name=\"target\")\n",
    "    \n",
    "    X_final = pd.concat([X_majority_reduced, X_minority], axis=0).reset_index(drop=True)\n",
    "    y_final = pd.concat([y_majority_reduced, y_minority], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Concat resampled data\n",
    "    reduced_data = pd.concat([X_final, y_final], axis=1)\n",
    "    \n",
    "    reduced_data[\"source\"] = None  # Initialize the source column with None\n",
    "    \n",
    "    # Compare data to copy source column\n",
    "    data_nosource = compare_df.drop(columns=[\"source\"])\n",
    "    reduced_data_nosource = reduced_data\n",
    "\n",
    "    # Iterate through the rows in reduced_data_nosource\n",
    "    for index, row in reduced_data_nosource.iterrows():\n",
    "        match = data_nosource.eq(row).all(axis=1)  # Check where rows are identical\n",
    "        if match.any():  # If a match is found\n",
    "            matched_index = match.idxmax()  # Get the first matching index\n",
    "            reduced_data.loc[index, \"source\"] = compare_df.loc[matched_index, \"source\"]\n",
    "            \n",
    "    # Check for any rows that still have None in the source column\n",
    "    missing_source = reduced_data[reduced_data[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        reduced_data.loc[reduced_data[\"source\"].isna(), \"source\"] = \"centroid\"       \n",
    "    \n",
    "    reduced_data.to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_KM_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Data reduced for {name} data\")\n",
    "    print(reduced_data[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClusterCentroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for mix data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for smote data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for GAN data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\mateu\\anaconda3\\envs\\python8\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reduced for borderline data\n",
      "target\n",
      "0    151\n",
      "1    151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "CC = ClusterCentroids(sampling_strategy=\"majority\", voting=\"auto\")\n",
    "\n",
    "for (name, (X_train, y_train)), (_, compare_df) in zip(data.items(), compare.items()):\n",
    "    \n",
    "    X_CC, y_CC = CC.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Concat resampled data\n",
    "    reduced_data = pd.concat([X_CC, y_CC], axis=1)\n",
    "    \n",
    "    # Compare data to copy source column\n",
    "    data_nosource = compare_df.drop(columns=[\"source\"])\n",
    "    reduced_data_nosource = reduced_data.drop(columns=[\"source\"], errors='ignore')\n",
    "\n",
    "    reduced_data[\"source\"] = None  # Initialize the source column with None\n",
    "\n",
    "    # Iterate through the rows in reduced_data_nosource\n",
    "    for index, row in reduced_data_nosource.iterrows():\n",
    "        match = data_nosource.eq(row).all(axis=1)  # Check where rows are identical\n",
    "        if match.any():  # If a match is found\n",
    "            matched_index = match.idxmax()  # Get the first matching index\n",
    "            reduced_data.loc[index, \"source\"] = compare_df.loc[matched_index, \"source\"]\n",
    "    \n",
    "    # Check for any rows that still have None in the source column\n",
    "    missing_source = reduced_data[reduced_data[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        reduced_data.loc[reduced_data[\"source\"].isna(), \"source\"] = \"centroid\"\n",
    "    \n",
    "    reduced_data.to_csv(f\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\reduced\\\\{name}_CC_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"Data reduced for {name} data\")\n",
    "    print(reduced_data[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "models[\"LR\"]        = LogisticRegression()\n",
    "models[\"LR_mix\"]    = LogisticRegression()\n",
    "models[\"LR_smote\"]  = LogisticRegression()\n",
    "models[\"LR_GAN\"]    = LogisticRegression()\n",
    "models[\"LR_borderline\"] = LogisticRegression()\n",
    "\n",
    "models[\"DT\"]        = DecisionTreeClassifier()\n",
    "models[\"DT_mix\"]    = DecisionTreeClassifier()\n",
    "models[\"DT_smote\"]  = DecisionTreeClassifier()\n",
    "models[\"DT_GAN\"]    = DecisionTreeClassifier()\n",
    "models[\"DT_borderline\"] = DecisionTreeClassifier()\n",
    "\n",
    "models[\"RF\"]        = RandomForestClassifier()\n",
    "models[\"RF_mix\"]    = RandomForestClassifier()\n",
    "models[\"RF_smote\"]  = RandomForestClassifier()\n",
    "models[\"RF_GAN\"]    = RandomForestClassifier()\n",
    "models[\"RF_borderline\"] = RandomForestClassifier()\n",
    "\n",
    "models[\"XGB\"]       = XGBClassifier()\n",
    "models[\"XGB_mix\"]   = XGBClassifier()\n",
    "models[\"XGB_smote\"] = XGBClassifier()\n",
    "models[\"XGB_GAN\"]   = XGBClassifier()\n",
    "models[\"XGB_borderline\"] = XGBClassifier()\n",
    "\n",
    "models[\"XGBRF\"]     = XGBRFClassifier()\n",
    "models[\"XGBRF_mix\"] = XGBRFClassifier()\n",
    "models[\"XGBRF_smote\"] = XGBRFClassifier()\n",
    "models[\"XGBRF_GAN\"] = XGBRFClassifier()\n",
    "models[\"XGBRF_borderline\"] = XGBRFClassifier()\n",
    "\n",
    "\n",
    "trainig_data = {}\n",
    "trainig_data[\"LR\"]=\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
