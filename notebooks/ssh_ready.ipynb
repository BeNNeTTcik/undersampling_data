{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118f9d56",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pickle\n",
    "import datacompy\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import gower\n",
    "from pathlib import Path\n",
    "\n",
    "# narzedzia\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    learning_curve,\n",
    "    RepeatedStratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, accuracy_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.spatial import distance\n",
    "from joblib import dump, load\n",
    "\n",
    "# modele\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# methods\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "#from ctgan import CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9455fa",
   "metadata": {},
   "source": [
    "### Read datasets after preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cefde3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling data\n",
    "original_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\original_data_normalized.csv\")\n",
    "mix_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\mix_data_normalized.csv\")\n",
    "smote_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\smote_data_normalized.csv\") \n",
    "gan_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\gan_data_normalized.csv\")\n",
    "borderline_df = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\encoded_normalized\\\\borderline_data_normalized.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"D:\\\\ml\\\\undersampling_data\\\\data\\\\ssh\\\\test\\\\X_test_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "292468a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0.0    151\n",
      "1.0     46\n",
      "Name: count, dtype: int64\n",
      "Before undersampling: 105\n",
      "After number of samples: 302\n"
     ]
    }
   ],
   "source": [
    "#Before undersampling\n",
    "print(original_df[\"target\"].value_counts())\n",
    "count1=original_df[\"target\"].value_counts().sum()\n",
    "count2=abs((original_df['target']==0).sum() - (original_df['target']==1).sum())\n",
    "print(f\"Before undersampling: {count2}\")\n",
    "print(f\"After number of samples: {count1+count2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum up all df \n",
    "sum_all_data = pd.concat([smote_df, gan_df, borderline_df, original_df], axis=0, ignore_index=True)\n",
    "\n",
    "#Split data\n",
    "X_mix, y_mix = mix_df.drop(columns=[\"target\", \"source\"]), mix_df[\"target\"]\n",
    "X_smote, y_smote = smote_df.drop(columns=[\"target\", \"source\"]), smote_df[\"target\"]\n",
    "X_GAN, y_GAN = gan_df.drop(columns=[\"target\", \"source\"]), gan_df[\"target\"]\n",
    "X_borderline, y_borderline = borderline_df.drop(columns=[\"target\", \"source\"]), borderline_df[\"target\"]\n",
    "\n",
    "#Dictionary\n",
    "data = {}\n",
    "data[\"mix\"] = (X_mix, y_mix)\n",
    "data[\"smote\"] = (X_smote, y_smote)\n",
    "data[\"GAN\"] = (X_GAN, y_GAN)\n",
    "data[\"borderline\"] = (X_borderline, y_borderline)\n",
    "\n",
    "compare = {}\n",
    "compare[\"mix\"] = mix_df\n",
    "compare[\"smote\"] = smote_df\n",
    "compare[\"GAN\"] = gan_df\n",
    "compare[\"borderline\"] = borderline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584d1d8",
   "metadata": {},
   "source": [
    "### Undersampling df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9d589",
   "metadata": {},
   "source": [
    "#### Kmeans + centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/ssh/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&centroids\")\n",
    "\n",
    "df_ = {}\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Create a DataFrame for centroids\n",
    "    X_majority_reduced = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    y_majority_reduced = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    \n",
    "    # Combine reduced majority class with original minority class\n",
    "    df_majority = pd.concat([X_majority_reduced, y_majority_reduced], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Add source column if not present\n",
    "    df_majority[\"source\"] = None\n",
    "    missing_source = df_majority[df_majority[\"source\"].isna()]\n",
    "    if not missing_source.empty:\n",
    "        df_majority.loc[df_majority[\"source\"].isna(), \"source\"] = \"centroid\" \n",
    "    \n",
    "    print(df_majority)\n",
    "    df_majority = df_majority.reindex(columns=original_df.columns, fill_value=0.0)\n",
    "    \n",
    "    # Combine with original minority class\n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True)  \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_centroids.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    user_cols = [\n",
    "    'kamran', 'student', 'root', 'admins', 'phoenix', 'piglet',\n",
    "    'rainbow', 'runner', 'sam', 'abc123', 'passwd', 'newpass',\n",
    "    'notused', 'Hockey', 'internet', 'asshole', 'Maddock', 'computer',\n",
    "    'Mickey', 'qwerty', 'fiction', 'orange', 'tigger', 'wheeling',\n",
    "    'mustang', 'admin', 'jennifer', 'money', 'Justin', 'chris',\n",
    "    'david', 'foobar', 'buster', 'harley', 'jordan', 'stupid',\n",
    "    'apple', 'fred', 'summer', 'sunshine', 'andrew', 'osamac',\n",
    "    'gta', 'adminx', 'gtta', 'osamax'\n",
    "    ]\n",
    "    df = df_[name]\n",
    "    # policz sumę wartości 1 w każdym wierszu\n",
    "    row_sums = df[user_cols].sum(axis=1)\n",
    "    mask = row_sums == 1\n",
    "\n",
    "    # jeśli chcesz sprawdzić globalnie:\n",
    "    all_ok = mask.all()\n",
    "\n",
    "    print(\"Czy każdy wiersz ma dokładnie jedno '1' w kolumnach 1–45? ->\", all_ok)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50a8cf",
   "metadata": {},
   "source": [
    "### KMeans + The Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/ssh/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&nn\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_NN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original minority class\n",
    "    for i in range(len(centroids_)):\n",
    "        if (len(cluster_data_[name][i])>1):     # If more than one sample in the cluster\n",
    "            dist_={}\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = euclidean(centroid, row)         # Colculate Euclidean distance\n",
    "                \n",
    "            min_key = min(dist_, key=dist_.get)\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[min_key]]    # Select the nearest neighbor to the centroid\n",
    "            \n",
    "        else:           # If only one sample in the cluster\n",
    "            results_KM_NN_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_NN_[name].values(), ignore_index=True)    \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_nn.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba08a9c",
   "metadata": {},
   "source": [
    "### KMeans + cosinus similarity         #nie działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b15306",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([10], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m         dist_[j] \u001b[38;5;241m=\u001b[39m cosine_similarity(centroid, row)[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]         \u001b[38;5;66;03m# Colculate Cosine similarity\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     min_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(dist_, key\u001b[38;5;241m=\u001b[39mdist_\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m---> 61\u001b[0m     results_KM_COS_[name][i] \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_data_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmin_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m   \u001b[38;5;66;03m# Select the nearest neighbor to the centroid\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:       \u001b[38;5;66;03m# If only one sample in the cluster\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     results_KM_COS_[name][i] \u001b[38;5;241m=\u001b[39m cluster_data_[name][i]\u001b[38;5;241m.\u001b[39miloc[[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([10], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/ssh/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&cos\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original majority class\n",
    "    for i in range(len(centroids_)):                \n",
    "        if (len(cluster_data_[name][i])>1):       # If more than one sample in the cluster\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                dist_[j] = cosine_similarity(centroid, row)         # Colculate Cosine similarity\n",
    "                \n",
    "            min_key = max(dist_, key=dist_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]   # Select the nearest neighbor to the centroid\n",
    "            \n",
    "        else:       # If only one sample in the cluster\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)       \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_cos.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9640be4",
   "metadata": {},
   "source": [
    "### KMeans + cos + Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c2da654",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m     index_map \u001b[38;5;241m=\u001b[39m {j: idx \u001b[38;5;28;01mfor\u001b[39;00m j, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_)}\n\u001b[0;32m     62\u001b[0m     cos_[j] \u001b[38;5;241m=\u001b[39m cosine_similarity(centroid, row)          \u001b[38;5;66;03m# using Cosine similarity\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     dist_[j] \u001b[38;5;241m=\u001b[39m distance\u001b[38;5;241m.\u001b[39mmahalanobis(centroid, row, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# using Mahalanobis distance\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     comb_score_[j] \u001b[38;5;241m=\u001b[39m (alfa\u001b[38;5;241m*\u001b[39mcos_[j] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malfa)\u001b[38;5;241m*\u001b[39mdist_[j])\n\u001b[0;32m     67\u001b[0m min_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(comb_score_, key\u001b[38;5;241m=\u001b[39mcomb_score_\u001b[38;5;241m.\u001b[39mget)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\linalg\\_linalg.py:615\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    612\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_singular, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    614\u001b[0m               over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 615\u001b[0m     ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\linalg\\_linalg.py:104\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "base = Path(\"D:/ml/undersampling_data/data/ssh/reduced\")\n",
    "base2 = Path(\"D:/ml/undersampling_data/models/kmeans&cos&mal\")\n",
    "\n",
    "cluster_data_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "centroids_ = {}\n",
    "\n",
    "results_ = {}\n",
    "df_ = {}\n",
    "\n",
    "results_KM_COS_MAN_ = {\n",
    "    \"mix\": {},\n",
    "    \"smote\": {},\n",
    "    \"GAN\": {},\n",
    "    \"borderline\": {}\n",
    "}\n",
    "\n",
    "alfa = 0.5  #wazenie\n",
    "\n",
    "for name in data.keys() & compare.keys():\n",
    "    # Read data from dictionary\n",
    "    X_train, y_train = data[name]\n",
    "    compare_df = compare[name]\n",
    "    \n",
    "    # Select majority class (synthetic samples)\n",
    "    X_majority = X_train[y_train == 1]\n",
    "        \n",
    "    # Apply KMeans clustering\n",
    "    KM = KMeans(n_clusters=(int)(count2))\n",
    "    kmeans = KM.fit(X_majority)\n",
    "    \n",
    "    # Save KMeans model\n",
    "    file_path2 = base2 / f\"{name}_kmeans_model.pkl\"\n",
    "    with open(file_path2, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Centroids + Add rows to dictionary\n",
    "    for i in range ((int)(count2)):\n",
    "        rows_in_cluster = X_majority[kmeans.labels_ == i] \n",
    "        cluster_data_[name][i] = rows_in_cluster\n",
    "        \n",
    "        centroids_ = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)\n",
    "    \n",
    "    # Original majority class\n",
    "    for i in range(len(centroids_)):               \n",
    "        if (len(cluster_data_[name][i])>1):       # If more than one sample in the cluster\n",
    "            cos_={}\n",
    "            dist_={}\n",
    "            comb_score_ = {} # results for cosine similarity and Mahalanobis distance\n",
    "            index_ = {}\n",
    "            centroid = centroids_.iloc[i]\n",
    "            centroid = centroid.values.reshape(1,-1)\n",
    "            for j in range(len(cluster_data_[name][i])):\n",
    "                index_ = list(cluster_data_[name][i].index)\n",
    "                row = cluster_data_[name][i].iloc[j]\n",
    "                row = row.values.reshape(1,-1)\n",
    "                index_map = {j: idx for j, idx in enumerate(index_)}\n",
    "                cos_[j] = cosine_similarity(centroid, row)          # using Cosine similarity\n",
    "                dist_[j] = distance.mahalanobis(centroid, row, np.linalg.inv(np.cov(X_train.T)))  # using Mahalanobis distance\n",
    "                \n",
    "                comb_score_[j] = (alfa*cos_[j] + (1-alfa)*dist_[j])\n",
    "                \n",
    "            min_key = min(comb_score_, key=comb_score_.get)\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[min_key]]\n",
    "            \n",
    "        else:\n",
    "            results_KM_COS_[name][i] = cluster_data_[name][i].iloc[[0]]\n",
    "        \n",
    "        results_[name] = pd.concat(results_KM_COS_[name].values(), ignore_index=True)   \n",
    "    \n",
    "    # Combine reduced majority class with original df\n",
    "    df_y_majority = pd.Series([1] * (int)(count2), name=\"target\") \n",
    "    df_majority = pd.concat([results_[name], df_y_majority], axis=1).reset_index(drop=True)\n",
    "    columns_ = list(df_majority.columns.values)\n",
    "    df_majority = df_majority.merge(sum_all_data, on=columns_, how=\"left\")\n",
    "     \n",
    "    df_[name] = pd.concat([df_majority, original_df], axis=0).reset_index(drop=True) \n",
    "    \n",
    "    print(df_[name].info())\n",
    "    \n",
    "    # Save to CSV if file does not exist\n",
    "    file_path = base / f\"{name}_KM_cos_mal.csv\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File exists.\")\n",
    "    else:\n",
    "        df_[name].to_csv(file_path, index=False)\n",
    "        print(\"File saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
